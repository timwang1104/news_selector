"""
AIè¯­ä¹‰å»é‡æœåŠ¡
ç”¨äºåœ¨AIç­›é€‰åè¿›è¡ŒåŸºäºè¯­ä¹‰ç†è§£çš„æ·±åº¦å»é‡
"""
import logging
from typing import List, Dict, Tuple, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass

from ..models.news import NewsArticle
from ..filters.base import CombinedFilterResult
# ç§»é™¤ä¸å­˜åœ¨çš„ai_serviceå¯¼å…¥

logger = logging.getLogger(__name__)


@dataclass
class SemanticGroup:
    """è¯­ä¹‰ç›¸ä¼¼æ–‡ç« ç»„"""
    core_topic: str  # æ ¸å¿ƒä¸»é¢˜
    key_entities: List[str]  # å…³é”®å®ä½“
    articles: List[CombinedFilterResult]  # æ–‡ç« åˆ—è¡¨
    similarity_scores: List[float]  # ç›¸ä¼¼åº¦åˆ†æ•°
    kept_article: Optional[CombinedFilterResult] = None  # ä¿ç•™çš„æ–‡ç« 
    removed_articles: List[CombinedFilterResult] = None  # å»é™¤çš„æ–‡ç« 

    def __post_init__(self):
        if self.removed_articles is None:
            self.removed_articles = []


class AISemanticDeduplicator:
    """AIè¯­ä¹‰å»é‡å™¨"""
    
    def __init__(self,
                 semantic_threshold: float = 0.85,
                 time_window_hours: int = 48,
                 max_group_size: int = 5):
        """
        åˆå§‹åŒ–AIè¯­ä¹‰å»é‡å™¨
        
        Args:
            semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
            time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            max_group_size: å•ä¸ªè¯­ä¹‰ç»„æœ€å¤§æ–‡ç« æ•°
        """
        self.semantic_threshold = semantic_threshold
        self.time_window_hours = time_window_hours
        self.max_group_size = max_group_size
        self.ai_client = self._create_ai_client()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'original_count': 0,
            'deduplicated_count': 0,
            'removed_count': 0,
            'semantic_groups': []
        }

    def _create_ai_client(self):
        """åˆ›å»ºAIå®¢æˆ·ç«¯"""
        try:
            from ..ai.factory import create_ai_client
            from ..config.filter_config import AIFilterConfig

            # åˆ›å»ºAIé…ç½®
            ai_config = AIFilterConfig(
                temperature=0.3,
                max_tokens=2000,
                timeout=60
            )

            # åˆ›å»ºAIå®¢æˆ·ç«¯
            client = create_ai_client(ai_config)
            print(f"   ğŸ¤– AIè¯­ä¹‰å»é‡å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            return client

        except Exception as e:
            logger.error(f"Failed to create AI client for semantic deduplication: {e}")
            print(f"   âŒ AIè¯­ä¹‰å»é‡å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            return None
    
    def semantic_deduplicate(self, articles: List[CombinedFilterResult]) -> List[CombinedFilterResult]:
        """
        æ‰§è¡ŒAIè¯­ä¹‰å»é‡
        
        Args:
            articles: ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–‡ç« åˆ—è¡¨
        """
        if not articles or len(articles) <= 1:
            return articles
        
        print(f"ğŸ§  å¼€å§‹AIè¯­ä¹‰å»é‡ï¼Œæ–‡ç« æ•°: {len(articles)}")
        logger.info(f"Starting AI semantic deduplication for {len(articles)} articles")
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = {
            'original_count': len(articles),
            'deduplicated_count': 0,
            'removed_count': 0,
            'semantic_groups': []
        }
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ—©çš„åœ¨å‰ï¼‰
        sorted_articles = sorted(articles, key=lambda x: x.article.published)
        
        # æ‰§è¡Œè¯­ä¹‰åˆ†ç»„
        semantic_groups = self._perform_semantic_grouping(sorted_articles)
        
        # ä»æ¯ç»„ä¸­é€‰æ‹©æœ€ä½³æ–‡ç« 
        deduplicated_articles = self._select_best_from_groups(semantic_groups)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['deduplicated_count'] = len(deduplicated_articles)
        self.stats['removed_count'] = self.stats['original_count'] - self.stats['deduplicated_count']
        
        print(f"âœ… AIè¯­ä¹‰å»é‡å®Œæˆ: åŸå§‹{self.stats['original_count']}ç¯‡ â†’ ä¿ç•™{len(deduplicated_articles)}ç¯‡ â†’ å»é™¤{self.stats['removed_count']}ç¯‡")
        logger.info(f"AI semantic deduplication completed: {self.stats['original_count']} â†’ {len(deduplicated_articles)} articles")
        
        return deduplicated_articles
    
    def _perform_semantic_grouping(self, articles: List[CombinedFilterResult]) -> List[SemanticGroup]:
        """æ‰§è¡Œè¯­ä¹‰åˆ†ç»„"""
        semantic_groups = []
        processed_indices = set()
        
        print(f"   ğŸ” å¼€å§‹è¯­ä¹‰åˆ†ç»„åˆ†æ...")
        
        for i, article in enumerate(articles):
            if i in processed_indices:
                continue
            
            print(f"   ğŸ“– åˆ†ææ–‡ç«  {i+1}/{len(articles)}: {article.article.title[:40]}...")
            
            # åˆ›å»ºæ–°çš„è¯­ä¹‰ç»„
            current_group = SemanticGroup(
                core_topic="",
                key_entities=[],
                articles=[article],
                similarity_scores=[1.0]  # è‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ä¸º1.0
            )
            
            group_indices = {i}
            
            # åœ¨æ—¶é—´çª—å£å†…æŸ¥æ‰¾è¯­ä¹‰ç›¸ä¼¼çš„æ–‡ç« 
            time_window_start = article.article.published
            time_window_end = time_window_start + timedelta(hours=self.time_window_hours)
            
            for j, other_article in enumerate(articles[i+1:], start=i+1):
                if j in processed_indices:
                    continue
                
                # æ£€æŸ¥æ—¶é—´çª—å£
                if other_article.article.published > time_window_end:
                    break
                
                # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                similarity = self._calculate_semantic_similarity(article, other_article)
                
                if similarity >= self.semantic_threshold:
                    current_group.articles.append(other_article)
                    current_group.similarity_scores.append(similarity)
                    group_indices.add(j)
                    print(f"     ğŸ”— å‘ç°è¯­ä¹‰ç›¸ä¼¼æ–‡ç« : {other_article.article.title[:30]}... (ç›¸ä¼¼åº¦: {similarity:.2f})")
            
            # å¦‚æœç»„å†…æœ‰å¤šç¯‡æ–‡ç« ï¼Œåˆ†ææ ¸å¿ƒä¸»é¢˜
            if len(current_group.articles) > 1:
                current_group.core_topic, current_group.key_entities = self._analyze_group_topic(current_group.articles)
                print(f"     ğŸ“Š è¯­ä¹‰ç»„: {current_group.core_topic} (å…±{len(current_group.articles)}ç¯‡æ–‡ç« )")
            
            semantic_groups.append(current_group)
            processed_indices.update(group_indices)
        
        print(f"   âœ… è¯­ä¹‰åˆ†ç»„å®Œæˆ: å‘ç°{len([g for g in semantic_groups if len(g.articles) > 1])}ä¸ªé‡å¤ç»„")
        return semantic_groups
    
    def _calculate_semantic_similarity(self, article1: CombinedFilterResult, article2: CombinedFilterResult) -> float:
        """è®¡ç®—ä¸¤ç¯‡æ–‡ç« çš„è¯­ä¹‰ç›¸ä¼¼åº¦"""
        try:
            if not self.ai_client:
                logger.warning("AI client not available for semantic similarity calculation")
                return 0.0

            # æ„å»ºæ¯”è¾ƒæç¤º
            prompt = f"""è¯·åˆ†æä»¥ä¸‹ä¸¤ç¯‡æ–°é—»æ–‡ç« çš„è¯­ä¹‰ç›¸ä¼¼åº¦ï¼Œåˆ¤æ–­å®ƒä»¬æ˜¯å¦æŠ¥é“çš„æ˜¯åŒä¸€ä¸ªæ ¸å¿ƒäº‹ä»¶æˆ–ä¸»é¢˜ã€‚

æ–‡ç« 1:
æ ‡é¢˜: {article1.article.title}
æ‘˜è¦: {article1.article.summary}

æ–‡ç« 2:
æ ‡é¢˜: {article2.article.title}
æ‘˜è¦: {article2.article.summary}

è¯·ä»ä»¥ä¸‹ç»´åº¦åˆ†æï¼š
1. æ ¸å¿ƒäº‹ä»¶æ˜¯å¦ç›¸åŒ
2. ä¸»è¦å®ä½“æ˜¯å¦ç›¸åŒï¼ˆäººç‰©ã€å…¬å¸ã€äº§å“ç­‰ï¼‰
3. æ—¶é—´èƒŒæ™¯æ˜¯å¦ç›¸åŒ
4. æ–°é—»ä»·å€¼æ˜¯å¦é‡å 

è¯·ç»™å‡º0-1ä¹‹é—´çš„ç›¸ä¼¼åº¦åˆ†æ•°ï¼Œå…¶ä¸­ï¼š
- 0.9-1.0: å®Œå…¨ç›¸åŒçš„äº‹ä»¶ï¼Œåªæ˜¯è¡¨è¿°ä¸åŒ
- 0.8-0.9: é«˜åº¦ç›¸å…³çš„äº‹ä»¶ï¼Œæ ¸å¿ƒå†…å®¹ç›¸åŒ
- 0.7-0.8: ç›¸å…³äº‹ä»¶ï¼Œä½†æœ‰ä¸åŒè§’åº¦
- 0.6-0.7: å¼±ç›¸å…³
- 0.0-0.6: ä¸ç›¸å…³

åªè¿”å›æ•°å­—åˆ†æ•°ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""

            # è°ƒç”¨AIå®¢æˆ·ç«¯
            response = self.ai_client._call_ai_api(prompt)

            # è§£æåˆ†æ•°
            try:
                score = float(response.strip())
                return max(0.0, min(1.0, score))  # ç¡®ä¿åœ¨0-1èŒƒå›´å†…
            except ValueError:
                logger.warning(f"Failed to parse similarity score: {response}")
                return 0.0

        except Exception as e:
            logger.error(f"Error calculating semantic similarity: {e}")
            return 0.0
    
    def _analyze_group_topic(self, articles: List[CombinedFilterResult]) -> Tuple[str, List[str]]:
        """åˆ†æè¯­ä¹‰ç»„çš„æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®å®ä½“"""
        try:
            if not self.ai_client:
                logger.warning("AI client not available for group topic analysis")
                return "æœªçŸ¥ä¸»é¢˜", []

            # æ„å»ºåˆ†ææç¤º
            titles = [article.article.title for article in articles]
            summaries = [article.article.summary for article in articles]

            prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–°é—»æ–‡ç« ç»„çš„æ ¸å¿ƒä¸»é¢˜å’Œå…³é”®å®ä½“ï¼š

æ–‡ç« æ ‡é¢˜ï¼š
{chr(10).join([f"{i+1}. {title}" for i, title in enumerate(titles)])}

æ–‡ç« æ‘˜è¦ï¼š
{chr(10).join([f"{i+1}. {summary}" for i, summary in enumerate(summaries)])}

è¯·æä¾›ï¼š
1. æ ¸å¿ƒä¸»é¢˜ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰
2. å…³é”®å®ä½“ï¼ˆäººç‰©ã€å…¬å¸ã€äº§å“ã€åœ°ç‚¹ç­‰ï¼Œç”¨é€—å·åˆ†éš”ï¼‰

æ ¼å¼ï¼š
ä¸»é¢˜ï¼š[æ ¸å¿ƒä¸»é¢˜]
å®ä½“ï¼š[å®ä½“1,å®ä½“2,å®ä½“3]"""

            # è°ƒç”¨AIå®¢æˆ·ç«¯
            response = self.ai_client._call_ai_api(prompt)

            # è§£æå“åº”
            lines = response.strip().split('\n')
            core_topic = ""
            key_entities = []

            for line in lines:
                if line.startswith('ä¸»é¢˜ï¼š'):
                    core_topic = line[3:].strip()
                elif line.startswith('å®ä½“ï¼š'):
                    entities_str = line[3:].strip()
                    key_entities = [e.strip() for e in entities_str.split(',') if e.strip()]

            return core_topic or "æœªçŸ¥ä¸»é¢˜", key_entities

        except Exception as e:
            logger.error(f"Error analyzing group topic: {e}")
            return "æœªçŸ¥ä¸»é¢˜", []
    
    def _select_best_from_groups(self, semantic_groups: List[SemanticGroup]) -> List[CombinedFilterResult]:
        """ä»æ¯ä¸ªè¯­ä¹‰ç»„ä¸­é€‰æ‹©æœ€ä½³æ–‡ç« """
        selected_articles = []
        
        for group in semantic_groups:
            if len(group.articles) == 1:
                # å•ç¯‡æ–‡ç« ç›´æ¥ä¿ç•™
                selected_articles.append(group.articles[0])
            else:
                # å¤šç¯‡æ–‡ç« é€‰æ‹©æœ€ä½³çš„
                best_article = self._select_best_article(group.articles)
                group.kept_article = best_article
                group.removed_articles = [a for a in group.articles if a != best_article]
                
                selected_articles.append(best_article)
                
                # è®°å½•è¯­ä¹‰ç»„ä¿¡æ¯
                self.stats['semantic_groups'].append({
                    'core_topic': group.core_topic,
                    'key_entities': group.key_entities,
                    'kept_article': {
                        'title': best_article.article.title,
                        'published': best_article.article.published.isoformat(),
                        'final_score': best_article.final_score
                    },
                    'removed_articles': [
                        {
                            'title': a.article.title,
                            'published': a.article.published.isoformat(),
                            'final_score': a.final_score
                        } for a in group.removed_articles
                    ],
                    'group_size': len(group.articles)
                })
        
        return selected_articles
    
    def _select_best_article(self, articles: List[CombinedFilterResult]) -> CombinedFilterResult:
        """ä»æ–‡ç« ç»„ä¸­é€‰æ‹©æœ€ä½³æ–‡ç« """
        # é€‰æ‹©ç­–ç•¥ï¼šç»¼åˆè€ƒè™‘å‘å¸ƒæ—¶é—´å’Œç­›é€‰åˆ†æ•°
        # 1. ä¼˜å…ˆé€‰æ‹©æœ€æ—©å‘å¸ƒçš„ï¼ˆæ—¶æ•ˆæ€§ï¼‰
        # 2. å¦‚æœæ—¶é—´å·®ä¸å¤§ï¼ˆ<2å°æ—¶ï¼‰ï¼Œé€‰æ‹©åˆ†æ•°æ›´é«˜çš„
        
        if len(articles) == 1:
            return articles[0]
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        sorted_by_time = sorted(articles, key=lambda x: x.article.published)
        earliest = sorted_by_time[0]
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ—¶é—´ç›¸è¿‘ä½†åˆ†æ•°æ›´é«˜çš„æ–‡ç« 
        for article in sorted_by_time[1:]:
            time_diff = (article.article.published - earliest.article.published).total_seconds() / 3600
            
            # å¦‚æœæ—¶é—´å·®å°äº2å°æ—¶ä¸”åˆ†æ•°æ˜æ˜¾æ›´é«˜ï¼Œé€‰æ‹©åˆ†æ•°é«˜çš„
            if time_diff < 2 and article.final_score > earliest.final_score + 0.1:
                return article
        
        # é»˜è®¤è¿”å›æœ€æ—©çš„
        return earliest
    
    def get_semantic_deduplication_stats(self) -> Dict:
        """è·å–AIè¯­ä¹‰å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'original_count': self.stats['original_count'],
            'deduplicated_count': self.stats['deduplicated_count'],
            'removed_count': self.stats['removed_count'],
            'semantic_deduplication_rate': (
                self.stats['removed_count'] / self.stats['original_count'] 
                if self.stats['original_count'] > 0 else 0
            ),
            'semantic_groups_count': len(self.stats['semantic_groups']),
            'semantic_groups': self.stats['semantic_groups'][:5]  # åªè¿”å›å‰5ä¸ªè¯­ä¹‰ç»„
        }


# å…¨å±€AIå»é‡å™¨å®ä¾‹
_ai_deduplicator_instance = None


def get_ai_deduplicator(semantic_threshold: float = 0.85,
                       time_window_hours: int = 48) -> AISemanticDeduplicator:
    """
    è·å–AIè¯­ä¹‰å»é‡å™¨å®ä¾‹
    
    Args:
        semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        time_window_hours: æ—¶é—´çª—å£
        
    Returns:
        AIè¯­ä¹‰å»é‡å™¨å®ä¾‹
    """
    global _ai_deduplicator_instance
    if _ai_deduplicator_instance is None:
        _ai_deduplicator_instance = AISemanticDeduplicator(
            semantic_threshold=semantic_threshold,
            time_window_hours=time_window_hours
        )
    return _ai_deduplicator_instance


def ai_semantic_deduplicate(articles: List[CombinedFilterResult],
                           semantic_threshold: float = 0.85,
                           time_window_hours: int = 48) -> Tuple[List[CombinedFilterResult], Dict]:
    """
    AIè¯­ä¹‰å»é‡çš„ä¾¿æ·å‡½æ•°
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        semantic_threshold: è¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        time_window_hours: æ—¶é—´çª—å£
        
    Returns:
        (å»é‡åçš„æ–‡ç« åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
    """
    deduplicator = get_ai_deduplicator(
        semantic_threshold=semantic_threshold,
        time_window_hours=time_window_hours
    )
    
    deduplicated_articles = deduplicator.semantic_deduplicate(articles)
    stats = deduplicator.get_semantic_deduplication_stats()
    
    return deduplicated_articles, stats
