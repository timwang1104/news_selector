"""
æ‰¹é‡ç­›é€‰æœåŠ¡ - ç®¡ç†å¤šä¸ªè®¢é˜…æºçš„è‡ªåŠ¨åˆ‡æ¢å’Œæ‰¹é‡ç­›é€‰
"""
import logging
import time
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Any, Callable
from concurrent.futures import ThreadPoolExecutor, as_completed

from ..models.news import NewsArticle
from ..models.rss import RSSFeed, RSSArticle
from ..services.custom_rss_service import CustomRSSService
from ..services.filter_service import get_filter_service, FilterProgressCallback
from ..filters.base import (
    FilterChainResult,
    SubscriptionFilterResult,
    BatchFilterResult,
    CombinedFilterResult
)

logger = logging.getLogger(__name__)



class BatchFilterProgressCallback:
    """æ‰¹é‡ç­›é€‰è¿›åº¦å›è°ƒæ¥å£"""
    
    def on_batch_start(self, total_subscriptions: int):
        """æ‰¹é‡ç­›é€‰å¼€å§‹"""
        pass
    
    def on_subscription_start(self, subscription, current: int, total: int):
        """å¼€å§‹å¤„ç†è®¢é˜…æº"""
        pass
    
    def on_subscription_fetch_complete(self, subscription, articles_count: int):
        """è®¢é˜…æºæ–‡ç« è·å–å®Œæˆ"""
        pass
    
    def on_subscription_filter_complete(self, subscription, selected_count: int):
        """è®¢é˜…æºç­›é€‰å®Œæˆ"""
        pass
    
    def on_subscription_error(self, subscription, error: str):
        """è®¢é˜…æºå¤„ç†é”™è¯¯"""
        pass
    
    def on_batch_complete(self, result: BatchFilterResult):
        """æ‰¹é‡ç­›é€‰å®Œæˆ"""
        pass


class BatchFilterConfig:
    """æ‰¹é‡ç­›é€‰é…ç½®"""
    
    def __init__(self):
        # è®¢é˜…æºç­›é€‰é…ç½®
        self.max_subscriptions: Optional[int] = None  # æœ€å¤§å¤„ç†è®¢é˜…æºæ•°é‡
        self.subscription_filter: Optional[Callable[[any], bool]] = None  # è®¢é˜…æºè¿‡æ»¤å‡½æ•°

        # æ–‡ç« è·å–é…ç½®
        self.articles_per_subscription: int = 50  # æ¯ä¸ªè®¢é˜…æºè·å–çš„æ–‡ç« æ•°é‡
        self.exclude_read: bool = True  # æ˜¯å¦æ’é™¤å·²è¯»æ–‡ç« 
        self.hours_back: Optional[int] = 24  # è·å–å¤šå°‘å°æ—¶å†…çš„æ–‡ç« 

        # ç­›é€‰é…ç½®
        self.filter_type: str = "chain"  # ç­›é€‰ç±»å‹: keyword, ai, chain
        self.enable_parallel: bool = True  # æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†
        self.max_workers: int = 3  # æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°

        # å»é‡é…ç½®
        self.enable_global_deduplication: bool = True  # æ˜¯å¦å¯ç”¨å…¨å±€å»é‡ï¼ˆè·¨æ–°é—»æºï¼‰
        self.deduplication_threshold: float = 0.8  # å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼
        self.deduplication_time_window: int = 72  # å»é‡æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰

        # AIè¯­ä¹‰å»é‡é…ç½®
        self.enable_ai_semantic_deduplication: bool = True  # æ˜¯å¦å¯ç”¨AIè¯­ä¹‰å»é‡
        self.ai_semantic_threshold: float = 0.85  # AIè¯­ä¹‰ç›¸ä¼¼åº¦é˜ˆå€¼
        self.ai_semantic_time_window: int = 48  # AIè¯­ä¹‰å»é‡æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰

        # ç»“æœé…ç½®
        self.max_results_per_subscription: Optional[int] = None  # æ¯ä¸ªè®¢é˜…æºæœ€å¤§ç»“æœæ•°
        self.min_score_threshold: Optional[float] = None  # æœ€å°åˆ†æ•°é˜ˆå€¼
        self.max_total_articles: Optional[int] = None  # æ‰¹é‡ç­›é€‰æœ€å¤§æ–‡ç« æ•°é™åˆ¶
        self.sort_by: str = "final_score"  # æ’åºæ–¹å¼: final_score, published, subscription
        self.group_by_subscription: bool = True  # æ˜¯å¦æŒ‰è®¢é˜…æºåˆ†ç»„æ˜¾ç¤º

    def set_ai_filter_rules(self, min_score: int = 20, max_articles: int = 30):
        """
        è®¾ç½®AIç­›é€‰è§„åˆ™çš„ä¾¿æ·æ–¹æ³•

        Args:
            min_score: AIç­›é€‰æœ€ä½åˆ†æ•°é˜ˆå€¼ï¼ˆé»˜è®¤20åˆ†ï¼‰
            max_articles: æ‰¹é‡ç­›é€‰æœ€å¤§æ–‡ç« æ•°é™åˆ¶ï¼ˆé»˜è®¤30ç¯‡ï¼‰
        """
        self.max_total_articles = max_articles
        print(f"ğŸ¯ è®¾ç½®AIç­›é€‰è§„åˆ™: æœ€ä½åˆ†æ•° {min_score} åˆ†ï¼Œæœ€å¤§æ–‡ç« æ•° {max_articles} ç¯‡")

        # åŒæ—¶æ›´æ–°FilterServiceçš„AIé…ç½®
        try:
            get_filter_service().update_config("ai", min_score_threshold=min_score, batch_max_articles=max_articles)
            print(f"âœ… AIç­›é€‰é…ç½®å·²æ›´æ–°")
        except Exception as e:
            print(f"âš ï¸  æ›´æ–°AIç­›é€‰é…ç½®å¤±è´¥: {e}")




class CustomRSSBatchFilterManager:
    """è‡ªå®šä¹‰RSSæ‰¹é‡ç­›é€‰ç®¡ç†å™¨"""

    def __init__(self):
        self.custom_rss_service = CustomRSSService()
        self.filter_service = get_filter_service()

    def filter_subscriptions_batch(self,
                                 config: BatchFilterConfig,
                                 callback: Optional[BatchFilterProgressCallback] = None) -> BatchFilterResult:
        """
        æ‰¹é‡ç­›é€‰è‡ªå®šä¹‰RSSè®¢é˜…æº

        Args:
            config: æ‰¹é‡ç­›é€‰é…ç½®
            callback: è¿›åº¦å›è°ƒ

        Returns:
            æ‰¹é‡ç­›é€‰ç»“æœ
        """
        start_time = datetime.now()

        # è·å–è‡ªå®šä¹‰RSSè®¢é˜…æºåˆ—è¡¨
        rss_feeds = self._get_filtered_rss_feeds(config)

        if not rss_feeds:
            logger.warning("No RSS feeds found for filtering")
            return BatchFilterResult(
                total_subscriptions=0,
                processed_subscriptions=0,
                processing_start_time=start_time,
                processing_end_time=datetime.now()
            )

        logger.info(f"Starting batch filtering for {len(rss_feeds)} RSS feeds")

        # åˆ›å»ºæ‰¹é‡ç»“æœå¯¹è±¡
        batch_result = BatchFilterResult(
            total_subscriptions=len(rss_feeds),
            processed_subscriptions=0,
            processing_start_time=start_time
        )

        # é€šçŸ¥æ‰¹é‡ç­›é€‰å¼€å§‹
        if callback:
            callback.on_batch_start(len(rss_feeds))

        # å¤„ç†è®¢é˜…æº - æ”¯æŒå…¨å±€å»é‡
        if config.enable_global_deduplication:
            self._process_rss_feeds_with_global_deduplication(rss_feeds, config, batch_result, callback)
        else:
            # ä¼ ç»Ÿå¤„ç†æ–¹å¼ï¼ˆæ¯ä¸ªæºå•ç‹¬å»é‡ï¼‰
            if config.enable_parallel:
                self._process_rss_feeds_parallel(rss_feeds, config, batch_result, callback)
            else:
                self._process_rss_feeds_sequential(rss_feeds, config, batch_result, callback)

        # å®Œæˆå¤„ç†
        batch_result.processing_end_time = datetime.now()

        # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
        self._calculate_batch_statistics(batch_result)

        # é€šçŸ¥æ‰¹é‡ç­›é€‰å®Œæˆ
        if callback:
            callback.on_batch_complete(batch_result)

        logger.info(f"Batch filtering completed: {batch_result.processed_subscriptions}/{batch_result.total_subscriptions} RSS feeds processed")

        return batch_result

    def _process_rss_feeds_with_global_deduplication(self,
                                                   feeds: List[RSSFeed],
                                                   config: BatchFilterConfig,
                                                   batch_result: BatchFilterResult,
                                                   callback: Optional[BatchFilterProgressCallback]):
        """ä½¿ç”¨å…¨å±€å»é‡å¤„ç†RSSè®¢é˜…æº"""
        logger.info(f"Starting global deduplication processing for {len(feeds)} RSS feeds")

        # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰æ–‡ç« 
        all_articles = []
        articles_by_source = {}  # è®°å½•æ¯ç¯‡æ–‡ç« æ¥è‡ªå“ªä¸ªæº

        if callback:
            callback.on_batch_start(len(feeds))

        print(f"ğŸ”„ ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æ‰€æœ‰æ–°é—»æºçš„æ–‡ç« ...")

        for i, feed in enumerate(feeds):
            try:
                if callback:
                    subscription = self._rss_feed_to_subscription(feed)
                    callback.on_subscription_start(subscription, i + 1, len(feeds))

                # è·å–æ–‡ç« ä½†ä¸è¿›è¡Œç­›é€‰
                articles = self._get_rss_feed_articles(feed, config)

                # è®°å½•æ–‡ç« æ¥æº
                for article in articles:
                    articles_by_source[article.id] = {
                        'feed_id': feed.id,
                        'feed_title': feed.title,
                        'article': article
                    }

                all_articles.extend(articles)

                print(f"   ğŸ“° {feed.title}: è·å– {len(articles)} ç¯‡æ–‡ç« ")

                if callback:
                    callback.on_subscription_filter_complete(feed, len(articles))

            except Exception as e:
                logger.error(f"Failed to fetch articles from feed {feed.title}: {e}")
                print(f"âŒ è·å–æ–‡ç« å¤±è´¥ {feed.title}: {e}")

                # åˆ›å»ºç©ºç»“æœ
                empty_result = SubscriptionFilterResult(
                    subscription_id=feed.id,
                    subscription_title=feed.title,
                    filter_result=FilterChainResult(total_articles=0, processing_start_time=datetime.now()),
                    articles_fetched=0,
                    fetch_time=0.0
                )
                batch_result.subscription_results.append(empty_result)
                batch_result.processed_subscriptions += 1

        print(f"âœ… æ”¶é›†å®Œæˆï¼šå…±è·å– {len(all_articles)} ç¯‡æ–‡ç« ")

        # ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€å»é‡å’Œç­›é€‰
        if all_articles:
            print(f"ğŸ”„ ç¬¬äºŒé˜¶æ®µï¼šå…¨å±€å»é‡å’Œç­›é€‰...")

            # é€šçŸ¥å¼€å§‹å…¨å±€å»é‡
            if callback and hasattr(callback, 'on_global_deduplication_start'):
                callback.on_global_deduplication_start(len(all_articles))

            # ä¸´æ—¶è®¾ç½®AIè¯­ä¹‰å»é‡é…ç½®åˆ°ç­›é€‰æœåŠ¡
            self._apply_ai_semantic_config_to_filter_service(config)

            # æ‰§è¡Œå…¨å±€ç­›é€‰ï¼ˆåŒ…å«å»é‡å’ŒAIè¯­ä¹‰å»é‡ï¼‰
            filter_result = self.filter_service.filter_articles(
                articles=all_articles,
                filter_type=config.filter_type,
                enable_deduplication=True  # å¼ºåˆ¶å¯ç”¨åŸºç¡€å»é‡
            )

            print(f"âœ… å…¨å±€ç­›é€‰å®Œæˆï¼š")
            print(f"   åŸå§‹æ–‡ç« ï¼š{filter_result.original_articles_count}")
            print(f"   å»é‡åï¼š{filter_result.deduplicated_articles_count}")
            print(f"   å»é™¤é‡å¤ï¼š{filter_result.removed_duplicates_count}")
            print(f"   æœ€ç»ˆç­›é€‰ï¼š{len(filter_result.selected_articles)}")

            # é€šçŸ¥å»é‡å®Œæˆ
            if callback and hasattr(callback, 'on_global_deduplication_complete'):
                callback.on_global_deduplication_complete(
                    filter_result.original_articles_count,
                    filter_result.deduplicated_articles_count,
                    filter_result.removed_duplicates_count
                )

            # é€šçŸ¥ç­›é€‰å®Œæˆ
            if callback and hasattr(callback, 'on_global_filtering_complete'):
                callback.on_global_filtering_complete(len(filter_result.selected_articles))

            # ç¬¬ä¸‰é˜¶æ®µï¼šæŒ‰æ¥æºåˆ†ç»„ç»“æœ
            print(f"ğŸ”„ ç¬¬ä¸‰é˜¶æ®µï¼šæŒ‰æ¥æºåˆ†ç»„ç»“æœ...")

            # é€šçŸ¥å¼€å§‹åˆ†ç»„
            if callback and hasattr(callback, 'on_result_distribution_start'):
                callback.on_result_distribution_start()

            self._distribute_results_by_source(filter_result, articles_by_source, feeds, batch_result, config)

            # é€šçŸ¥åˆ†ç»„å®Œæˆ
            if callback and hasattr(callback, 'on_result_distribution_complete'):
                callback.on_result_distribution_complete()

        else:
            print(f"âš ï¸  æ²¡æœ‰è·å–åˆ°ä»»ä½•æ–‡ç« ")
            # ä¸ºæ‰€æœ‰è®¢é˜…æºåˆ›å»ºç©ºç»“æœ
            for feed in feeds:
                empty_result = SubscriptionFilterResult(
                    subscription_id=feed.id,
                    subscription_title=feed.title,
                    filter_result=FilterChainResult(total_articles=0, processing_start_time=datetime.now()),
                    articles_fetched=0,
                    fetch_time=0.0
                )
                batch_result.subscription_results.append(empty_result)
                batch_result.processed_subscriptions += 1

    def _distribute_results_by_source(self,
                                    global_filter_result: FilterChainResult,
                                    articles_by_source: Dict,
                                    feeds: List[RSSFeed],
                                    batch_result: BatchFilterResult,
                                    config: BatchFilterConfig):
        """å°†å…¨å±€ç­›é€‰ç»“æœæŒ‰æ¥æºåˆ†ç»„"""

        # ä¸ºæ¯ä¸ªè®¢é˜…æºåˆ›å»ºç»“æœç»Ÿè®¡
        source_stats = {}
        for feed in feeds:
            source_stats[feed.id] = {
                'feed': feed,
                'original_articles': 0,
                'selected_articles': [],
                'rejected_articles': []
            }

        # ç»Ÿè®¡åŸå§‹æ–‡ç« æ•°
        for article_id, source_info in articles_by_source.items():
            feed_id = source_info['feed_id']
            if feed_id in source_stats:
                source_stats[feed_id]['original_articles'] += 1

        # åˆ†é…ç­›é€‰é€šè¿‡çš„æ–‡ç« 
        for combined_result in global_filter_result.selected_articles:
            # CombinedFilterResultå¯¹è±¡ï¼Œéœ€è¦è®¿é—®å…¶articleå±æ€§
            article_id = combined_result.article.id
            if article_id in articles_by_source:
                source_info = articles_by_source[article_id]
                feed_id = source_info['feed_id']
                if feed_id in source_stats:
                    source_stats[feed_id]['selected_articles'].append(combined_result)

        # åˆ†é…è¢«æ‹’ç»çš„æ–‡ç« ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if hasattr(global_filter_result, 'rejected_articles') and global_filter_result.rejected_articles:
            for combined_result in global_filter_result.rejected_articles:
                # CombinedFilterResultå¯¹è±¡ï¼Œéœ€è¦è®¿é—®å…¶articleå±æ€§
                article_id = combined_result.article.id
                if article_id in articles_by_source:
                    source_info = articles_by_source[article_id]
                    feed_id = source_info['feed_id']
                    if feed_id in source_stats:
                        source_stats[feed_id]['rejected_articles'].append(combined_result)

        # ä¸ºæ¯ä¸ªè®¢é˜…æºåˆ›å»ºSubscriptionFilterResult
        for feed_id, stats in source_stats.items():
            feed = stats['feed']

            # åˆ›å»ºè¯¥æºçš„ç­›é€‰ç»“æœ
            source_filter_result = FilterChainResult(
                total_articles=stats['original_articles'],
                processing_start_time=global_filter_result.processing_start_time
            )

            # è®¾ç½®é€‰ä¸­çš„æ–‡ç« 
            source_filter_result.selected_articles = stats['selected_articles']

            # å¤åˆ¶å…¨å±€å»é‡ç»Ÿè®¡ä¿¡æ¯
            source_filter_result.original_articles_count = global_filter_result.original_articles_count
            source_filter_result.deduplicated_articles_count = global_filter_result.deduplicated_articles_count
            source_filter_result.removed_duplicates_count = global_filter_result.removed_duplicates_count
            source_filter_result.deduplication_stats = global_filter_result.deduplication_stats

            # è®¾ç½®ç­›é€‰ç»Ÿè®¡
            source_filter_result.final_selected_count = len(stats['selected_articles'])
            source_filter_result.processing_end_time = global_filter_result.processing_end_time

            # åº”ç”¨ç»“æœè¿‡æ»¤
            self._apply_result_filters(source_filter_result, config)

            # åˆ›å»ºè®¢é˜…æºç»“æœ
            subscription_result = SubscriptionFilterResult(
                subscription_id=feed.id,
                subscription_title=feed.title,
                filter_result=source_filter_result,
                articles_fetched=stats['original_articles'],
                fetch_time=0.0  # å…¨å±€å¤„ç†æ¨¡å¼ä¸‹æ— å•ç‹¬è·å–æ—¶é—´
            )

            batch_result.subscription_results.append(subscription_result)
            batch_result.processed_subscriptions += 1

            print(f"   ğŸ“Š {feed.title}: åŸå§‹{stats['original_articles']}ç¯‡ â†’ ç­›é€‰{len(stats['selected_articles'])}ç¯‡")

    def _apply_ai_semantic_config_to_filter_service(self, batch_config: BatchFilterConfig):
        """å°†AIè¯­ä¹‰å»é‡é…ç½®åº”ç”¨åˆ°ç­›é€‰æœåŠ¡"""
        try:
            # è·å–ç­›é€‰æœåŠ¡çš„ç­›é€‰é“¾
            filter_chain = self.filter_service.filter_chain
            if not filter_chain:
                print(f"   âš ï¸  ç­›é€‰é“¾æœªåˆå§‹åŒ–ï¼Œè·³è¿‡AIè¯­ä¹‰å»é‡é…ç½®")
                return

            # ä»æ‰¹é‡é…ç½®ä¸­è¯»å–AIè¯­ä¹‰å»é‡è®¾ç½®
            enable_ai_semantic = getattr(batch_config, 'enable_ai_semantic_deduplication', True)
            ai_semantic_threshold = getattr(batch_config, 'ai_semantic_threshold', 0.85)
            ai_semantic_time_window = getattr(batch_config, 'ai_semantic_time_window', 48)

            # ä¹Ÿä»é…ç½®æ–‡ä»¶è¯»å–AIè¯­ä¹‰å»é‡è®¾ç½®
            try:
                import json
                from pathlib import Path

                config_file = Path("config/filter_config.json")
                if config_file.exists():
                    with open(config_file, 'r', encoding='utf-8') as f:
                        file_config = json.load(f)

                        # è¯»å–AIè¯­ä¹‰å»é‡é…ç½®
                        if 'ai_semantic_deduplication' in file_config:
                            ai_dedup_config = file_config['ai_semantic_deduplication']
                            enable_ai_semantic = ai_dedup_config.get('enabled', enable_ai_semantic)
                            ai_semantic_threshold = ai_dedup_config.get('threshold', ai_semantic_threshold)
                            ai_semantic_time_window = ai_dedup_config.get('time_window_hours', ai_semantic_time_window)

                        # è¯»å–chainé…ç½®ä¸­çš„AIè¯­ä¹‰å»é‡è®¾ç½®
                        if 'chain' in file_config:
                            chain_config = file_config['chain']
                            if 'enable_ai_semantic_deduplication' in chain_config:
                                enable_ai_semantic = chain_config['enable_ai_semantic_deduplication']
                            if 'ai_semantic_threshold' in chain_config:
                                ai_semantic_threshold = chain_config['ai_semantic_threshold']
                            if 'ai_semantic_time_window' in chain_config:
                                ai_semantic_time_window = chain_config['ai_semantic_time_window']

            except Exception as e:
                logger.warning(f"Failed to load AI semantic deduplication config from file: {e}")

            # åº”ç”¨é…ç½®åˆ°ç­›é€‰é“¾
            if hasattr(filter_chain, 'config'):
                filter_chain.config.enable_ai_semantic_deduplication = enable_ai_semantic
                filter_chain.config.ai_semantic_threshold = ai_semantic_threshold
                filter_chain.config.ai_semantic_time_window = ai_semantic_time_window

            print(f"   ğŸ§  AIè¯­ä¹‰å»é‡é…ç½®: å¯ç”¨={enable_ai_semantic}, é˜ˆå€¼={ai_semantic_threshold}, çª—å£={ai_semantic_time_window}å°æ—¶")

        except Exception as e:
            logger.error(f"Failed to apply AI semantic config to filter service: {e}")
            print(f"   âŒ AIè¯­ä¹‰å»é‡é…ç½®åº”ç”¨å¤±è´¥: {e}")

    def _get_filtered_rss_feeds(self, config: BatchFilterConfig) -> List[RSSFeed]:
        """è·å–è¿‡æ»¤åçš„RSSè®¢é˜…æºåˆ—è¡¨"""
        try:
            all_feeds = self.custom_rss_service.get_active_subscriptions()

            # åº”ç”¨è®¢é˜…æºè¿‡æ»¤å™¨
            filtered_feeds = []

            for feed in all_feeds:
                # è‡ªå®šä¹‰è¿‡æ»¤å‡½æ•°
                if config.subscription_filter and not config.subscription_filter(feed):
                    continue

                # ç›´æ¥æ·»åŠ æ‰€æœ‰é€šè¿‡è‡ªå®šä¹‰è¿‡æ»¤å™¨çš„è®¢é˜…æº
                filtered_feeds.append(feed)

            # é™åˆ¶æ•°é‡
            if config.max_subscriptions:
                filtered_feeds = filtered_feeds[:config.max_subscriptions]

            logger.info(f"Filtered {len(filtered_feeds)} RSS feeds from {len(all_feeds)} total")
            return filtered_feeds

        except Exception as e:
            logger.error(f"Failed to get RSS feeds: {e}")
            return []



    def _process_rss_feeds_sequential(self,
                                    feeds: List[RSSFeed],
                                    config: BatchFilterConfig,
                                    batch_result: BatchFilterResult,
                                    callback: Optional[BatchFilterProgressCallback]):
        """é¡ºåºå¤„ç†RSSè®¢é˜…æº"""
        for i, feed in enumerate(feeds):
            try:
                # æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æ–‡ç« æ•°é‡é™åˆ¶
                if config.max_total_articles:
                    current_total = sum(r.filter_result.final_selected_count for r in batch_result.subscription_results)
                    if current_total >= config.max_total_articles:
                        print(f"ğŸ›‘ æ‰¹é‡ç­›é€‰æå‰é€€å‡º: å·²è¾¾åˆ°æ–‡ç« æ•°é‡é™åˆ¶ {config.max_total_articles} ç¯‡")
                        logger.info(f"Batch filtering stopped: reached max articles limit ({config.max_total_articles})")
                        batch_result.warnings.append(f"å·²è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°é‡é™åˆ¶ {config.max_total_articles}ï¼Œæå‰é€€å‡ºç­›é€‰")
                        break

                if callback:
                    subscription = self._rss_feed_to_subscription(feed)
                    callback.on_subscription_start(subscription, i + 1, len(feeds))

                result = self._process_single_rss_feed(feed, config, callback)
                batch_result.subscription_results.append(result)
                batch_result.processed_subscriptions += 1

                # å¤„ç†å®Œæˆåå†æ¬¡æ£€æŸ¥æ–‡ç« æ•°é‡
                if config.max_total_articles:
                    current_total = sum(r.filter_result.final_selected_count for r in batch_result.subscription_results)
                    print(f"ğŸ“Š å½“å‰ç´¯è®¡ç­›é€‰æ–‡ç« æ•°: {current_total}/{config.max_total_articles}")
                    if current_total >= config.max_total_articles:
                        print(f"ğŸ›‘ æ‰¹é‡ç­›é€‰è¾¾åˆ°é™åˆ¶: ç´¯è®¡ç­›é€‰äº† {current_total} ç¯‡æ–‡ç« ")
                        logger.info(f"Batch filtering completed: reached max articles limit ({current_total} articles)")
                        break

            except Exception as e:
                logger.error(f"Error processing RSS feed {feed.title}: {e}")
                batch_result.errors.append(f"RSS feed {feed.title}: {e}")

    def _process_rss_feeds_parallel(self,
                                  feeds: List[RSSFeed],
                                  config: BatchFilterConfig,
                                  batch_result: BatchFilterResult,
                                  callback: Optional[BatchFilterProgressCallback]):
        """å¹¶è¡Œå¤„ç†RSSè®¢é˜…æº"""
        # å¦‚æœè®¾ç½®äº†æ–‡ç« æ•°é‡é™åˆ¶ï¼Œä½¿ç”¨é¡ºåºå¤„ç†ä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶
        if config.max_total_articles:
            print(f"âš ï¸  æ£€æµ‹åˆ°æ–‡ç« æ•°é‡é™åˆ¶ ({config.max_total_articles})ï¼Œåˆ‡æ¢åˆ°é¡ºåºå¤„ç†æ¨¡å¼")
            logger.info(f"Switching to sequential processing due to max articles limit: {config.max_total_articles}")
            self._process_rss_feeds_sequential(feeds, config, batch_result, callback)
            return

        with ThreadPoolExecutor(max_workers=config.max_workers) as executor:
            # æäº¤ä»»åŠ¡
            future_to_feed = {
                executor.submit(self._process_single_rss_feed, feed, config, callback): feed
                for feed in feeds
            }

            # æ”¶é›†ç»“æœ
            for i, future in enumerate(as_completed(future_to_feed)):
                feed = future_to_feed[future]
                try:
                    if callback:
                        subscription = self._rss_feed_to_subscription(feed)
                        callback.on_subscription_start(subscription, i + 1, len(feeds))

                    result = future.result()
                    batch_result.subscription_results.append(result)
                    batch_result.processed_subscriptions += 1

                except Exception as e:
                    logger.error(f"Error processing RSS feed {feed.title}: {e}")
                    batch_result.errors.append(f"RSS feed {feed.title}: {e}")

    def _process_single_rss_feed(self,
                               feed: RSSFeed,
                               config: BatchFilterConfig,
                               callback: Optional[BatchFilterProgressCallback]) -> SubscriptionFilterResult:
        """å¤„ç†å•ä¸ªRSSè®¢é˜…æº"""
        logger.debug(f"Processing RSS feed: {feed.title}")

        # è·å–æ–‡ç« 
        fetch_start = time.time()
        articles = self._get_rss_feed_articles(feed, config)
        fetch_time = time.time() - fetch_start

        if callback:
            subscription = self._rss_feed_to_subscription(feed)
            callback.on_subscription_fetch_complete(subscription, len(articles))

        # ç­›é€‰æ–‡ç« 
        if articles:
            filter_result = self.filter_service.filter_articles(
                articles=articles,
                filter_type=config.filter_type
            )

            # åº”ç”¨ç»“æœè¿‡æ»¤
            self._apply_result_filters(filter_result, config)
        else:
            # åˆ›å»ºç©ºçš„ç­›é€‰ç»“æœ
            filter_result = FilterChainResult(
                total_articles=0,
                processing_start_time=datetime.now()
            )
            filter_result.processing_end_time = datetime.now()

        if callback:
            callback.on_subscription_filter_complete(feed, filter_result.final_selected_count)

        return SubscriptionFilterResult(
            subscription_id=feed.id,
            subscription_title=feed.title,
            filter_result=filter_result,
            articles_fetched=len(articles),
            fetch_time=fetch_time
        )

    def _get_rss_feed_articles(self, feed: RSSFeed, config: BatchFilterConfig) -> List[NewsArticle]:
        """è·å–RSSè®¢é˜…æºçš„æ–‡ç« """
        try:
            # ä»RSS feedä¸­è·å–æ–‡ç« 
            rss_articles = feed.articles

            # åº”ç”¨æ—¶é—´è¿‡æ»¤
            if config.hours_back:
                cutoff_time = datetime.now(timezone.utc) - timedelta(hours=config.hours_back)
                rss_articles = [
                    article for article in rss_articles
                    if article.published >= cutoff_time
                ]

            # åº”ç”¨å·²è¯»è¿‡æ»¤
            if config.exclude_read:
                rss_articles = [
                    article for article in rss_articles
                    if not article.is_read
                ]

            # é™åˆ¶æ•°é‡
            if config.articles_per_subscription:
                rss_articles = rss_articles[:config.articles_per_subscription]

            # è½¬æ¢ä¸ºNewsArticleå¯¹è±¡
            news_articles = []
            for rss_article in rss_articles:
                news_article = self._rss_article_to_news_article(rss_article, feed)
                news_articles.append(news_article)

            return news_articles

        except Exception as e:
            logger.error(f"Failed to get articles from RSS feed {feed.title}: {e}")
            return []

    def _rss_article_to_news_article(self, rss_article: RSSArticle, feed: RSSFeed) -> NewsArticle:
        """å°†RSSArticleè½¬æ¢ä¸ºNewsArticleå¯¹è±¡"""
        # éœ€è¦æ·»åŠ updatedå­—æ®µï¼Œå¦‚æœRSSæ–‡ç« æ²¡æœ‰updatedæ—¶é—´ï¼Œä½¿ç”¨publishedæ—¶é—´
        updated_time = getattr(rss_article, 'updated', None) or rss_article.published

        # å¤„ç†ä½œè€…ä¿¡æ¯
        author = None
        if rss_article.author:
            from ..models.news import NewsAuthor
            author = NewsAuthor(name=rss_article.author)

        return NewsArticle(
            id=rss_article.id,
            title=rss_article.title,
            summary=rss_article.summary,
            content=rss_article.content,
            url=rss_article.url,
            published=rss_article.published,
            updated=updated_time,
            author=author,
            categories=[],  # RSSæ–‡ç« é€šå¸¸æ²¡æœ‰åˆ†ç±»ä¿¡æ¯
            is_read=rss_article.is_read,
            is_starred=False,  # RSSæ–‡ç« é€šå¸¸æ²¡æœ‰æ˜Ÿæ ‡åŠŸèƒ½
            feed_id=feed.id,
            feed_title=feed.title
        )

    def _apply_result_filters(self, filter_result: FilterChainResult, config: BatchFilterConfig):
        """åº”ç”¨ç»“æœè¿‡æ»¤é…ç½®"""
        # åº”ç”¨æœ€å°åˆ†æ•°é˜ˆå€¼
        if config.min_score_threshold is not None:
            if hasattr(filter_result, 'final_results') and filter_result.final_results:
                filter_result.final_results = [
                    result for result in filter_result.final_results
                    if getattr(result, 'final_score', 0) >= config.min_score_threshold
                ]
                filter_result.final_selected_count = len(filter_result.final_results)

        # åº”ç”¨æ¯ä¸ªè®¢é˜…æºæœ€å¤§ç»“æœæ•°é™åˆ¶
        if config.max_results_per_subscription is not None:
            if hasattr(filter_result, 'final_results') and filter_result.final_results:
                # æŒ‰åˆ†æ•°æ’åºå¹¶å–å‰Nä¸ª
                if hasattr(filter_result.final_results[0], 'final_score'):
                    filter_result.final_results.sort(
                        key=lambda x: getattr(x, 'final_score', 0),
                        reverse=True
                    )
                filter_result.final_results = filter_result.final_results[:config.max_results_per_subscription]
                filter_result.final_selected_count = len(filter_result.final_results)

    def _rss_feed_to_subscription(self, feed: RSSFeed):
        """å°†RSS Feedè½¬æ¢ä¸ºè®¢é˜…æºå¯¹è±¡ï¼ˆç”¨äºå›è°ƒï¼‰"""
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„è®¢é˜…æºå¯¹è±¡ï¼ŒåŒ…å«å¿…è¦çš„ä¿¡æ¯
        class SimpleSubscription:
            def __init__(self, feed: RSSFeed):
                self.id = feed.id
                self.title = feed.title
                self.url = feed.url
                self.category = getattr(feed, 'category', 'é»˜è®¤')
                self.description = getattr(feed, 'description', '')

        return SimpleSubscription(feed)

    def _calculate_batch_statistics(self, batch_result: BatchFilterResult):
        """è®¡ç®—æ‰¹é‡ç­›é€‰ç»Ÿè®¡ä¿¡æ¯"""
        batch_result.total_articles_fetched = sum(
            result.articles_fetched for result in batch_result.subscription_results
        )
        batch_result.total_articles_selected = sum(
            result.filter_result.final_selected_count for result in batch_result.subscription_results
        )

        # total_processing_timeæ˜¯åªè¯»å±æ€§ï¼Œç”±processing_end_time - processing_start_timeè‡ªåŠ¨è®¡ç®—
        # ä¸éœ€è¦æ‰‹åŠ¨è®¾ç½®


# å…¨å±€æ‰¹é‡ç­›é€‰ç®¡ç†å™¨å®ä¾‹
custom_rss_batch_filter_manager = CustomRSSBatchFilterManager()
