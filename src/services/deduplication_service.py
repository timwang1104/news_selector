"""
æ–°é—»å»é‡æœåŠ¡
"""
import logging
import re
from typing import List, Dict, Set, Tuple, Optional
from datetime import datetime, timedelta
from difflib import SequenceMatcher
from collections import defaultdict

from ..models.news import NewsArticle

logger = logging.getLogger(__name__)


class NewsDeduplicator:
    """æ–°é—»å»é‡å™¨"""
    
    def __init__(self,
                 title_threshold: float = 0.8,
                 content_threshold: float = 0.7,
                 time_window_hours: int = 72,
                 min_content_length: int = 20):
        """
        åˆå§‹åŒ–å»é‡å™¨
        
        Args:
            title_threshold: æ ‡é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
            content_threshold: å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼
            time_window_hours: æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
            min_content_length: æœ€å°å†…å®¹é•¿åº¦
        """
        self.title_threshold = title_threshold
        self.content_threshold = content_threshold
        self.time_window_hours = time_window_hours
        self.min_content_length = min_content_length
        
        # å»é‡ç»Ÿè®¡
        self.stats = {
            'original_count': 0,
            'deduplicated_count': 0,
            'removed_count': 0,
            'duplicate_groups': []
        }
    
    def deduplicate(self, articles: List[NewsArticle]) -> List[NewsArticle]:
        """
        æ‰§è¡Œå»é‡
        
        Args:
            articles: åŸå§‹æ–‡ç« åˆ—è¡¨
            
        Returns:
            å»é‡åçš„æ–‡ç« åˆ—è¡¨
        """
        if not articles:
            return articles
        
        logger.info(f"å¼€å§‹å»é‡ï¼ŒåŸå§‹æ–‡ç« æ•°: {len(articles)}")
        
        # é‡ç½®ç»Ÿè®¡
        self.stats = {
            'original_count': len(articles),
            'deduplicated_count': 0,
            'removed_count': 0,
            'duplicate_groups': []
        }
        
        # é¢„å¤„ç†æ–‡ç« 
        processed_articles = self._preprocess_articles(articles)
        
        # æŒ‰æ—¶é—´æ’åºï¼ˆæœ€æ—©çš„åœ¨å‰ï¼‰
        processed_articles.sort(key=lambda x: x.published)
        
        # æ‰§è¡Œå»é‡
        deduplicated = self._perform_deduplication(processed_articles)
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['deduplicated_count'] = len(deduplicated)
        self.stats['removed_count'] = self.stats['original_count'] - self.stats['deduplicated_count']
        
        logger.info(f"å»é‡å®Œæˆï¼Œä¿ç•™æ–‡ç« æ•°: {len(deduplicated)}, å»é™¤é‡å¤: {self.stats['removed_count']}")
        
        return deduplicated
    
    def _preprocess_articles(self, articles: List[NewsArticle]) -> List[NewsArticle]:
        """é¢„å¤„ç†æ–‡ç« """
        processed = []
        
        for article in articles:
            # è¿‡æ»¤æ‰å†…å®¹è¿‡çŸ­çš„æ–‡ç« 
            if len(article.content) < self.min_content_length:
                continue
            
            # æ ‡å‡†åŒ–æ–‡æœ¬
            article.title = self._normalize_text(article.title)
            article.summary = self._normalize_text(article.summary)
            article.content = self._normalize_text(article.content)
            
            processed.append(article)
        
        return processed
    
    def _normalize_text(self, text: str) -> str:
        """æ ‡å‡†åŒ–æ–‡æœ¬"""
        if not text:
            return ""
        
        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text.strip())
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆä¿ç•™ä¸­è‹±æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹ï¼‰
        text = re.sub(r'[^\w\s\u4e00-\u9fff.,!?;:()ï¼ˆï¼‰ã€ã€‘""''â€”-]', '', text)
        
        return text
    
    def _perform_deduplication(self, articles: List[NewsArticle]) -> List[NewsArticle]:
        """æ‰§è¡Œå»é‡é€»è¾‘"""
        if not articles:
            return articles
        
        # ç”¨äºå­˜å‚¨å»é‡åçš„æ–‡ç« 
        deduplicated = []
        # ç”¨äºè·Ÿè¸ªå·²å¤„ç†çš„æ–‡ç« ç´¢å¼•
        processed_indices = set()

        print(f"     ğŸ” å¼€å§‹å»é‡ç®—æ³•ï¼Œæ–‡ç« æ•°: {len(articles)}, æ—¶é—´çª—å£: {self.time_window_hours}å°æ—¶, ç›¸ä¼¼åº¦é˜ˆå€¼: {self.title_threshold}")

        # æ¯å¤„ç†10ç¯‡æ–‡ç« æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        progress_interval = max(1, len(articles) // 10)

        for i, article in enumerate(articles):
            if i in processed_indices:
                continue

            # æ˜¾ç¤ºè¿›åº¦
            if i % progress_interval == 0 or i == len(articles) - 1:
                print(f"     ğŸ“– å¤„ç†è¿›åº¦: {i+1}/{len(articles)} ({(i+1)/len(articles)*100:.1f}%)")

            # å½“å‰æ–‡ç« ä½œä¸ºå€™é€‰ä¿ç•™æ–‡ç« 
            current_group = [article]
            duplicate_indices = {i}
            
            # åœ¨æ—¶é—´çª—å£å†…æŸ¥æ‰¾ç›¸ä¼¼æ–‡ç« 
            time_window_start = article.published
            time_window_end = time_window_start + timedelta(hours=self.time_window_hours)
            
            for j, other_article in enumerate(articles[i+1:], start=i+1):
                if j in processed_indices:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦åœ¨æ—¶é—´çª—å£å†…
                if other_article.published > time_window_end:
                    break  # ç”±äºå·²æ’åºï¼Œåç»­æ–‡ç« éƒ½è¶…å‡ºæ—¶é—´çª—å£
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = self._calculate_similarity(article, other_article)
                
                if similarity >= self.title_threshold:
                    current_group.append(other_article)
                    duplicate_indices.add(j)
            
            # å¦‚æœæ‰¾åˆ°é‡å¤æ–‡ç« ï¼Œè®°å½•é‡å¤ç»„
            if len(current_group) > 1:
                self.stats['duplicate_groups'].append({
                    'kept_article': {
                        'title': article.title,
                        'published': article.published.isoformat(),
                        'url': article.url
                    },
                    'removed_articles': [
                        {
                            'title': dup.title,
                            'published': dup.published.isoformat(),
                            'url': dup.url
                        }
                        for dup in current_group[1:]
                    ],
                    'similarity_scores': [
                        self._calculate_similarity(article, dup)
                        for dup in current_group[1:]
                    ]
                })
            
            # ä¿ç•™æœ€æ—©å‘å¸ƒçš„æ–‡ç« ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼Œç¬¬ä¸€ä¸ªå°±æ˜¯æœ€æ—©çš„ï¼‰
            deduplicated.append(article)
            
            # æ ‡è®°æ‰€æœ‰é‡å¤æ–‡ç« ä¸ºå·²å¤„ç†
            processed_indices.update(duplicate_indices)
        
        return deduplicated
    
    def _calculate_similarity(self, article1: NewsArticle, article2: NewsArticle) -> float:
        """
        è®¡ç®—ä¸¤ç¯‡æ–‡ç« çš„ç›¸ä¼¼åº¦
        
        Args:
            article1: æ–‡ç« 1
            article2: æ–‡ç« 2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        # æ ‡é¢˜ç›¸ä¼¼åº¦ï¼ˆæƒé‡0.7ï¼‰
        title_sim = self._text_similarity(article1.title, article2.title)
        
        # æ‘˜è¦ç›¸ä¼¼åº¦ï¼ˆæƒé‡0.2ï¼‰
        summary_sim = self._text_similarity(article1.summary, article2.summary)
        
        # å†…å®¹ç›¸ä¼¼åº¦ï¼ˆæƒé‡0.1ï¼Œåªå–å‰500å­—ç¬¦é¿å…è®¡ç®—é‡è¿‡å¤§ï¼‰
        content1 = article1.content[:500] if article1.content else ""
        content2 = article2.content[:500] if article2.content else ""
        content_sim = self._text_similarity(content1, content2)
        
        # åŠ æƒå¹³å‡
        weighted_similarity = (
            title_sim * 0.7 + 
            summary_sim * 0.2 + 
            content_sim * 0.1
        )
        
        return weighted_similarity
    
    def _text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦
        
        Args:
            text1: æ–‡æœ¬1
            text2: æ–‡æœ¬2
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        if not text1 or not text2:
            return 0.0
        
        # ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        matcher = SequenceMatcher(None, text1, text2)
        return matcher.ratio()
    
    def get_deduplication_stats(self) -> Dict:
        """è·å–å»é‡ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'original_count': self.stats['original_count'],
            'deduplicated_count': self.stats['deduplicated_count'],
            'removed_count': self.stats['removed_count'],
            'deduplication_rate': (
                self.stats['removed_count'] / self.stats['original_count'] 
                if self.stats['original_count'] > 0 else 0
            ),
            'duplicate_groups_count': len(self.stats['duplicate_groups']),
            'duplicate_groups': self.stats['duplicate_groups'][:10]  # åªè¿”å›å‰10ä¸ªé‡å¤ç»„
        }
    
    def get_detailed_stats(self) -> Dict:
        """è·å–è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.get_deduplication_stats()
        
        # æ·»åŠ æ›´å¤šç»Ÿè®¡ä¿¡æ¯
        if self.stats['duplicate_groups']:
            similarities = []
            for group in self.stats['duplicate_groups']:
                similarities.extend(group['similarity_scores'])
            
            stats.update({
                'avg_similarity': sum(similarities) / len(similarities) if similarities else 0,
                'max_similarity': max(similarities) if similarities else 0,
                'min_similarity': min(similarities) if similarities else 0,
                'total_duplicates_found': sum(len(group['removed_articles']) for group in self.stats['duplicate_groups'])
            })
        
        return stats


# å…¨å±€å»é‡å™¨å®ä¾‹
_deduplicator_instance = None


def get_deduplicator(title_threshold: float = 0.8,
                    content_threshold: float = 0.7,
                    time_window_hours: int = 72) -> NewsDeduplicator:
    """
    è·å–å»é‡å™¨å®ä¾‹
    
    Args:
        title_threshold: æ ‡é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
        content_threshold: å†…å®¹ç›¸ä¼¼åº¦é˜ˆå€¼
        time_window_hours: æ—¶é—´çª—å£
        
    Returns:
        å»é‡å™¨å®ä¾‹
    """
    global _deduplicator_instance
    if _deduplicator_instance is None:
        _deduplicator_instance = NewsDeduplicator(
            title_threshold=title_threshold,
            content_threshold=content_threshold,
            time_window_hours=time_window_hours
        )
    return _deduplicator_instance


def deduplicate_articles(articles: List[NewsArticle], 
                        title_threshold: float = 0.8,
                        time_window_hours: int = 72) -> Tuple[List[NewsArticle], Dict]:
    """
    å»é‡æ–‡ç« çš„ä¾¿æ·å‡½æ•°
    
    Args:
        articles: æ–‡ç« åˆ—è¡¨
        title_threshold: æ ‡é¢˜ç›¸ä¼¼åº¦é˜ˆå€¼
        time_window_hours: æ—¶é—´çª—å£
        
    Returns:
        (å»é‡åçš„æ–‡ç« åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯)
    """
    deduplicator = get_deduplicator(
        title_threshold=title_threshold,
        time_window_hours=time_window_hours
    )
    
    deduplicated_articles = deduplicator.deduplicate(articles)
    stats = deduplicator.get_deduplication_stats()
    
    return deduplicated_articles, stats


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("æ–°é—»å»é‡æœåŠ¡æµ‹è¯•")
    
    deduplicator = get_deduplicator()
    print(f"å»é‡å™¨é…ç½®: æ ‡é¢˜é˜ˆå€¼={deduplicator.title_threshold}, æ—¶é—´çª—å£={deduplicator.time_window_hours}å°æ—¶")
