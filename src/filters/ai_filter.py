"""
AIæ™ºèƒ½ç­›é€‰å™¨å®ç°
"""
import time
import logging
from typing import List, Optional
from ..models.news import NewsArticle
from ..config.filter_config import AIFilterConfig
from ..ai.factory import create_ai_client
from ..ai.exceptions import AIClientError
from ..ai.cache import AIResultCache
# å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯å¯¼å…¥
# from ..utils.ai_analysis_storage import ai_analysis_storage
from .base import BaseFilter, AIFilterResult, FilterMetrics, AIEvaluation

logger = logging.getLogger(__name__)


class AIFilter(BaseFilter):
    """AIæ™ºèƒ½ç­›é€‰å™¨"""
    
    def __init__(self, config: AIFilterConfig):
        self.config = config
        self.client = create_ai_client(config)
        self.cache = AIResultCache(ttl=config.cache_ttl, max_size=config.cache_size) if config.enable_cache else None
        self.metrics = FilterMetrics()
    
    def filter(self, articles: List[NewsArticle]) -> List[AIFilterResult]:
        """ç­›é€‰æ–‡ç« åˆ—è¡¨"""
        if not articles:
            return []

        print(f"ğŸ¤– AIç­›é€‰å¼€å§‹: å‡†å¤‡å¤„ç† {len(articles)} ç¯‡æ–‡ç« ")
        for i, article in enumerate(articles):
            print(f"   å¾…ç­›é€‰æ–‡ç« {i+1}: {article.title}")

        # é™åˆ¶å¤„ç†æ•°é‡
        if len(articles) > self.config.max_requests:
            logger.warning(f"Too many articles ({len(articles)}), limiting to {self.config.max_requests}")
            print(f"âš ï¸  æ–‡ç« æ•°é‡è¶…é™ï¼Œé™åˆ¶ä¸ºå‰ {self.config.max_requests} ç¯‡")
            articles = articles[:self.config.max_requests]

        results = []

        # æ‰¹é‡å¤„ç†
        for batch_num, batch in enumerate(self._create_batches(articles, self.config.batch_size)):
            print(f"ğŸ”„ å¤„ç†ç¬¬ {batch_num + 1} æ‰¹: {len(batch)} ç¯‡æ–‡ç« ")
            batch_results = self._process_batch(batch)
            results.extend(batch_results)

        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ‰€æœ‰è¯„åˆ†
        print(f"ğŸ” AIç­›é€‰è¯¦æƒ…: æ€»å…±å¤„ç† {len(articles)} ç¯‡æ–‡ç« ï¼Œè·å¾— {len(results)} ä¸ªæœ‰æ•ˆç»“æœ")
        if results:
            scores = [r.evaluation.total_score for r in results]
            print(f"ğŸ“Š AIè¯„åˆ†åˆ†å¸ƒ: æœ€é«˜={max(scores):.1f}, æœ€ä½={min(scores):.1f}, å¹³å‡={sum(scores)/len(scores):.1f}")
            for i, result in enumerate(results[:5]):  # æ˜¾ç¤ºå‰5ä¸ªç»“æœ
                print(f"   #{i+1}: åˆ†æ•°={result.evaluation.total_score:.1f}, æ ‡é¢˜={result.article.title[:50]}...")
        else:
            print(f"âš ï¸  AIç­›é€‰æ— æœ‰æ•ˆç»“æœ: å¯èƒ½æ˜¯APIè°ƒç”¨å¤±è´¥æˆ–æ‰€æœ‰æ–‡ç« è¯„åˆ†è¿‡ä½")

        # æŒ‰æ€»åˆ†æ’åº
        results.sort(key=lambda x: x.evaluation.total_score, reverse=True)

        # æ–°çš„ç­›é€‰é€»è¾‘ï¼šåŸºäºåˆ†æ•°é˜ˆå€¼ç­›é€‰
        min_score_threshold = getattr(self.config, 'min_score_threshold', 20)  # é»˜è®¤20åˆ†

        # ç­›é€‰è¶…è¿‡é˜ˆå€¼åˆ†æ•°çš„æ–‡ç« 
        selected_results = [r for r in results if r.evaluation.total_score >= min_score_threshold]

        print(f"ğŸ¯ åˆ†æ•°é˜ˆå€¼ç­›é€‰: {len(selected_results)} ç¯‡æ–‡ç« è¶…è¿‡ {min_score_threshold} åˆ†é˜ˆå€¼")
        print(f"âœ… AIç­›é€‰æœ€ç»ˆç»“æœ: é€‰æ‹©äº† {len(selected_results)} ç¯‡è¶…è¿‡ {min_score_threshold} åˆ†çš„æ–‡ç« ")
        logger.info(f"AIç­›é€‰å®Œæˆ: å¤„ç†äº†{len(results)}ç¯‡æ–‡ç« ï¼Œæœ€ç»ˆé€‰æ‹©äº†{len(selected_results)}ç¯‡è¶…è¿‡{min_score_threshold}åˆ†é˜ˆå€¼çš„æ–‡ç« ")

        return selected_results
    
    def filter_single(self, article: NewsArticle) -> Optional[AIFilterResult]:
        """ç­›é€‰å•ç¯‡æ–‡ç« """
        start_time = time.time()
        article_title = article.title[:60] + "..." if len(article.title) > 60 else article.title

        logger.debug(f"å¼€å§‹AIç­›é€‰: {article_title}")

        try:
            # æ£€æŸ¥ç¼“å­˜
            cached_evaluation = None
            if self.cache:
                logger.debug(f"æ£€æŸ¥ç¼“å­˜: {article_title}")
                cached_evaluation = self.cache.get(article)
                if cached_evaluation:
                    self.metrics.record_cache_hit()
                    processing_time = time.time() - start_time

                    logger.info(f"ç¼“å­˜å‘½ä¸­: {article_title} - è¯„åˆ†: {cached_evaluation.total_score}/30 (ç¼“å­˜)")

                    cached_result = AIFilterResult(
                        article=article,
                        evaluation=cached_evaluation,
                        processing_time=processing_time,
                        ai_model=self.config.model_name,
                        cached=True
                    )

                    # æ£€æŸ¥æ˜¯å¦å·²æœ‰åˆ†æç»“æœå­˜å‚¨ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¿å­˜ï¼ˆç¼“å­˜ç»“æœå¯èƒ½æ²¡æœ‰åŸå§‹å“åº”ï¼‰
                    try:
                        from ..utils.ai_analysis_storage import ai_analysis_storage
                        if not ai_analysis_storage.has_analysis(article):
                            ai_analysis_storage.save_analysis(article, cached_result, "ç¼“å­˜ç»“æœï¼Œæ— åŸå§‹å“åº”")
                            logger.debug(f"ç¼“å­˜ç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨: {article_title}")
                    except Exception as e:
                        logger.warning(f"ä¿å­˜ç¼“å­˜ç»“æœå¤±è´¥: {e}")

                    return cached_result
                else:
                    self.metrics.record_cache_miss()
                    logger.debug(f"ç¼“å­˜æœªå‘½ä¸­: {article_title}")

            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµ‹è¯•æ¨¡å¼
            if self.config.test_mode:
                logger.debug(f"æµ‹è¯•æ¨¡å¼: ç”Ÿæˆæ¨¡æ‹Ÿè¯„ä¼°æ•°æ® - {article_title}")
                evaluation, raw_response = self._generate_test_evaluation(article)
                # æ¨¡æ‹ŸAIè°ƒç”¨å»¶è¿Ÿ
                time.sleep(self.config.test_mode_delay)
            else:
                # AIè¯„ä¼°ï¼ˆè·å–åŸå§‹å“åº”ï¼‰
                logger.debug(f"å¼€å§‹AIè¯„ä¼°: {article_title}")

                # å°è¯•è·å–åŸå§‹å“åº”
                raw_response = ""
                if hasattr(self.client, 'evaluate_article_with_raw_response'):
                    try:
                        evaluation, raw_response = self.client.evaluate_article_with_raw_response(article)
                    except Exception as e:
                        logger.warning(f"è·å–åŸå§‹å“åº”å¤±è´¥ï¼Œé™çº§åˆ°æ™®é€šè¯„ä¼°: {e}")
                        evaluation = self.client.evaluate_article(article)
                else:
                    evaluation = self.client.evaluate_article(article)

            # è®°å½•è¯„ä¼°è¯¦æƒ…
            logger.info(f"AIè¯„ä¼°å®Œæˆ: {article_title}")
            logger.info(f"  æ”¿ç­–ç›¸å…³æ€§: {evaluation.relevance_score}/10")
            logger.info(f"  åˆ›æ–°å½±å“: {evaluation.innovation_impact}/10")
            logger.info(f"  å®ç”¨æ€§: {evaluation.practicality}/10")
            logger.info(f"  æ€»åˆ†: {evaluation.total_score}/30")
            logger.info(f"  ç½®ä¿¡åº¦: {evaluation.confidence:.2f}")
            if evaluation.reasoning:
                logger.debug(f"  è¯„ä¼°ç†ç”±: {evaluation.reasoning[:200]}...")

            # ç¼“å­˜ç»“æœ
            if self.cache and evaluation.confidence >= self.config.min_confidence:
                self.cache.set(article, evaluation)
                logger.debug(f"ç»“æœå·²ç¼“å­˜: {article_title}")

            processing_time = time.time() - start_time
            self.metrics.record_processing_time(processing_time * 1000)

            # åˆ¤æ–­è¯„ä¼°è´¨é‡
            if evaluation.total_score >= 20:
                quality_level = "ä¼˜ç§€"
            elif evaluation.total_score >= 15:
                quality_level = "è‰¯å¥½"
            elif evaluation.total_score >= 10:
                quality_level = "ä¸€èˆ¬"
            else:
                quality_level = "è¾ƒå·®"

            logger.info(f"è¯„ä¼°è´¨é‡: {quality_level} (è€—æ—¶: {processing_time:.2f}ç§’)")

            result = AIFilterResult(
                article=article,
                evaluation=evaluation,
                processing_time=processing_time,
                ai_model=self.config.model_name,
                cached=False
            )

            # ä¿å­˜AIåˆ†æç»“æœåˆ°æœ¬åœ°å­˜å‚¨
            try:
                from ..utils.ai_analysis_storage import ai_analysis_storage
                ai_analysis_storage.save_analysis(article, result, raw_response)
                logger.debug(f"AIåˆ†æç»“æœå·²ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨: {article_title}")
            except Exception as e:
                logger.warning(f"ä¿å­˜AIåˆ†æç»“æœå¤±è´¥: {e}")

            # è¿”å›è¯„ä¼°ç»“æœï¼Œç”±è°ƒç”¨æ–¹è¿›è¡Œæ’åç­›é€‰
            return result

        except AIClientError as e:
            self.metrics.record_error()
            logger.error(f"AIç­›é€‰å¤±è´¥: {article_title} - {e}")

            # é™çº§ç­–ç•¥
            if self.config.fallback_enabled:
                logger.warning(f"å¯ç”¨é™çº§ç­–ç•¥: {article_title}")
                return self._fallback_filter(article, start_time)
            else:
                logger.error(f"é™çº§ç­–ç•¥å·²ç¦ç”¨ï¼Œè·³è¿‡æ–‡ç« : {article_title}")
                return None
        except Exception as e:
            self.metrics.record_error()
            logger.error(f"AIç­›é€‰å¼‚å¸¸: {article_title} - {e}")
            return None
    
    def _process_batch(self, articles: List[NewsArticle]) -> List[AIFilterResult]:
        """å¤„ç†æ–‡ç« æ‰¹æ¬¡"""
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµ‹è¯•æ¨¡å¼
        if self.config.test_mode:
            print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šæ‰¹é‡ç”Ÿæˆ {len(articles)} ç¯‡æ–‡ç« çš„æ¨¡æ‹Ÿè¯„ä¼°æ•°æ®")
            test_results = []
            start_time = time.time()
            
            for i, article in enumerate(articles):
                evaluation, _ = self._generate_test_evaluation(article)
                print(f"   æµ‹è¯•æ–‡ç« {i+1}: åˆ†æ•°={evaluation.total_score:.1f}, ç½®ä¿¡åº¦={evaluation.confidence:.2f}, æ ‡é¢˜={article.title[:40]}...")
                
                # åˆ›å»ºAIç­›é€‰ç»“æœ
                ai_result = AIFilterResult(
                    article=article,
                    evaluation=evaluation,
                    processing_time=self.config.test_mode_delay,
                    ai_model="test_mode",
                    cached=False
                )
                
                # ä¿å­˜æµ‹è¯•åˆ†æç»“æœåˆ°æœ¬åœ°å­˜å‚¨
                try:
                    from ..utils.ai_analysis_storage import ai_analysis_storage
                    ai_analysis_storage.save_analysis(article, ai_result, "æµ‹è¯•æ¨¡å¼æ‰¹é‡è¯„ä¼°")
                except Exception as e:
                    logger.warning(f"ä¿å­˜æµ‹è¯•AIåˆ†æç»“æœå¤±è´¥: {e}")
                
                test_results.append(ai_result)
                
                # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
                if self.config.test_mode_delay > 0:
                    time.sleep(self.config.test_mode_delay)
            
            processing_time = time.time() - start_time
            print(f"âœ… æµ‹è¯•æ¨¡å¼æ‰¹é‡è¯„ä¼°å®Œæˆ: è€—æ—¶ {processing_time:.2f}s, ç”Ÿæˆ {len(test_results)} ä¸ªæ¨¡æ‹Ÿç»“æœ")
            self.metrics.record_processing_time(processing_time * 1000)
            
            return test_results
        
        # æ£€æŸ¥ç¼“å­˜ï¼Œåˆ†ç¦»å·²ç¼“å­˜å’Œæœªç¼“å­˜çš„æ–‡ç« 
        cached_results = []
        uncached_articles = []
        
        if self.cache:
            for article in articles:
                cached_evaluation = self.cache.get(article)
                if cached_evaluation:
                    self.metrics.record_cache_hit()
                    cached_results.append(AIFilterResult(
                        article=article,
                        evaluation=cached_evaluation,
                        processing_time=0.0,
                        ai_model=self.config.model_name,
                        cached=True
                    ))
                else:
                    self.metrics.record_cache_miss()
                    uncached_articles.append(article)
        else:
            uncached_articles = articles
        
        # æ‰¹é‡è¯„ä¼°æœªç¼“å­˜çš„æ–‡ç« 
        uncached_results = []
        if uncached_articles:
            print(f"ğŸ¤– å¼€å§‹AIæ‰¹é‡è¯„ä¼°: {len(uncached_articles)} ç¯‡æ–‡ç« ")
            try:
                start_time = time.time()
                evaluations = self.client.batch_evaluate(uncached_articles)
                processing_time = time.time() - start_time

                print(f"âœ… AIæ‰¹é‡è¯„ä¼°å®Œæˆ: è€—æ—¶ {processing_time:.2f}s, è·å¾— {len(evaluations)} ä¸ªè¯„ä¼°ç»“æœ")

                # ç¡®ä¿è¯„ä¼°ç»“æœæ•°é‡ä¸æ–‡ç« æ•°é‡åŒ¹é…
                if len(evaluations) != len(uncached_articles):
                    print(f"âš ï¸  è¯„ä¼°ç»“æœæ•°é‡({len(evaluations)})ä¸æ–‡ç« æ•°é‡({len(uncached_articles)})ä¸åŒ¹é…")
                    # å¦‚æœè¯„ä¼°ç»“æœä¸è¶³ï¼Œç”¨é™çº§è¯„ä¼°è¡¥å……
                    while len(evaluations) < len(uncached_articles):
                        missing_article = uncached_articles[len(evaluations)]
                        fallback_eval = self._create_fallback_evaluation(missing_article)
                        evaluations.append(fallback_eval)
                        print(f"   ä¸ºæ–‡ç«  '{missing_article.title[:40]}...' æ·»åŠ é™çº§è¯„ä¼°")

                for i, (article, evaluation) in enumerate(zip(uncached_articles, evaluations)):
                    if evaluation:
                        print(f"   æ–‡ç« {i+1}: åˆ†æ•°={evaluation.total_score:.1f}, ç½®ä¿¡åº¦={evaluation.confidence:.2f}, æ ‡é¢˜={article.title[:40]}...")

                        # ç¼“å­˜ç»“æœ
                        if self.cache and evaluation.confidence >= self.config.min_confidence:
                            self.cache.set(article, evaluation)

                        # åˆ›å»ºAIç­›é€‰ç»“æœ
                        ai_result = AIFilterResult(
                            article=article,
                            evaluation=evaluation,
                            processing_time=processing_time / len(uncached_articles),
                            ai_model=self.config.model_name,
                            cached=False
                        )

                        # ä¿å­˜AIåˆ†æç»“æœåˆ°æœ¬åœ°å­˜å‚¨
                        try:
                            from ..utils.ai_analysis_storage import ai_analysis_storage
                            ai_analysis_storage.save_analysis(article, ai_result, "æ‰¹é‡è¯„ä¼°ï¼Œæ— åŸå§‹å“åº”")
                        except Exception as e:
                            logger.warning(f"ä¿å­˜æ‰¹é‡AIåˆ†æç»“æœå¤±è´¥: {e}")

                        uncached_results.append(ai_result)
                    else:
                        print(f"   æ–‡ç« {i+1}: è¯„ä¼°å¤±è´¥ï¼Œè·³è¿‡ - {article.title[:40]}...")

                self.metrics.record_processing_time(processing_time * 1000)

            except AIClientError as e:
                self.metrics.record_error()
                print(f"âŒ AIæ‰¹é‡è¯„ä¼°å¤±è´¥: {e}")
                logger.error(f"Batch AI evaluation failed: {e}")

                # é™çº§ç­–ç•¥
                if self.config.fallback_enabled:
                    print(f"ğŸ”„ å¯ç”¨é™çº§ç­–ç•¥")
                    for article in uncached_articles:
                        fallback_result = self._fallback_filter(article, time.time())
                        if fallback_result:
                            uncached_results.append(fallback_result)
            except Exception as e:
                print(f"âŒ AIè¯„ä¼°å¼‚å¸¸: {e}")
                logger.error(f"AI evaluation exception: {e}")
        
        return cached_results + uncached_results
    
    def _fallback_filter(self, article: NewsArticle, start_time: float) -> Optional[AIFilterResult]:
        """é™çº§ç­›é€‰ç­–ç•¥"""
        article_title = article.title[:60] + "..." if len(article.title) > 60 else article.title

        logger.info(f"æ‰§è¡Œé™çº§è¯„ä¼°: {article_title}")

        # ä½¿ç”¨ç®€å•çš„å¯å‘å¼è¯„ä¼°
        fallback_evaluation = self.client._fallback_evaluation(article)

        processing_time = time.time() - start_time

        logger.info(f"é™çº§è¯„ä¼°å®Œæˆ: {article_title} - è¯„åˆ†: {fallback_evaluation.total_score}/30 (é™çº§)")
        logger.debug(f"é™çº§ç†ç”±: {fallback_evaluation.reasoning}")

        return AIFilterResult(
            article=article,
            evaluation=fallback_evaluation,
            processing_time=processing_time,
            ai_model="fallback",
            cached=False
        )
    
    def _create_batches(self, articles: List[NewsArticle], batch_size: int) -> List[List[NewsArticle]]:
        """åˆ›å»ºæ–‡ç« æ‰¹æ¬¡"""
        batches = []
        for i in range(0, len(articles), batch_size):
            batch = articles[i:i + batch_size]
            batches.append(batch)
        return batches

    def _create_fallback_evaluation(self, article: NewsArticle) -> AIEvaluation:
        """åˆ›å»ºé™çº§è¯„ä¼°ç»“æœ"""
        return AIEvaluation(
            relevance_score=5,
            innovation_impact=5,
            practicality=5,
            total_score=15,
            reasoning="AIæ‰¹é‡è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é™çº§è¯„ä¼°ç­–ç•¥",
            confidence=0.3,
            summary="",
            key_insights=[],
            highlights=[],
            tags=[],
            detailed_analysis={},
            recommendation_reason="",
            risk_assessment="",
            implementation_suggestions=[]
        )
    
    def _generate_test_evaluation(self, article: NewsArticle) -> tuple[AIEvaluation, str]:
        """ç”Ÿæˆæµ‹è¯•æ¨¡å¼çš„æ¨¡æ‹Ÿè¯„ä¼°æ•°æ®"""
        import random
        import hashlib
        
        # åŸºäºæ–‡ç« æ ‡é¢˜ç”Ÿæˆç¡®å®šæ€§çš„éšæœºç§å­ï¼Œç¡®ä¿åŒä¸€æ–‡ç« æ€»æ˜¯å¾—åˆ°ç›¸åŒçš„è¯„ä¼°ç»“æœ
        seed = int(hashlib.md5(article.title.encode()).hexdigest()[:8], 16)
        random.seed(seed)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿè¯„åˆ†ï¼ˆåŸºäºæ–‡ç« æ ‡é¢˜ä¸­çš„å…³é”®è¯è¿›è¡Œç®€å•å¯å‘å¼è¯„ä¼°ï¼‰
        title_lower = article.title.lower()
        
        # åŸºç¡€åˆ†æ•°
        base_relevance = random.randint(4, 8)
        base_innovation = random.randint(4, 8)
        base_practicality = random.randint(4, 8)
        
        # æ ¹æ®æ ‡é¢˜å…³é”®è¯è°ƒæ•´åˆ†æ•°
        tech_keywords = ['ai', 'artificial intelligence', 'äººå·¥æ™ºèƒ½', 'quantum', 'é‡å­', 'biotech', 'ç”Ÿç‰©æŠ€æœ¯', 
                        'semiconductor', 'åŠå¯¼ä½“', '5g', '6g', 'cybersecurity', 'ç½‘ç»œå®‰å…¨', 'blockchain', 'åŒºå—é“¾']
        policy_keywords = ['policy', 'æ”¿ç­–', 'regulation', 'ç›‘ç®¡', 'government', 'æ”¿åºœ', 'strategy', 'æˆ˜ç•¥']
        innovation_keywords = ['innovation', 'åˆ›æ–°', 'breakthrough', 'çªç ´', 'research', 'ç ”ç©¶', 'development', 'å‘å±•']
        
        # æŠ€æœ¯ç›¸å…³æ€§åŠ åˆ†
        tech_bonus = sum(2 for keyword in tech_keywords if keyword in title_lower)
        policy_bonus = sum(1 for keyword in policy_keywords if keyword in title_lower)
        innovation_bonus = sum(1 for keyword in innovation_keywords if keyword in title_lower)
        
        relevance_score = min(10, base_relevance + tech_bonus + policy_bonus)
        innovation_impact = min(10, base_innovation + innovation_bonus + tech_bonus // 2)
        practicality = min(10, base_practicality + policy_bonus)
        
        total_score = relevance_score + innovation_impact + practicality
        confidence = random.uniform(0.7, 0.95)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„è¯„ä¼°ç†ç”±
        reasoning_templates = [
            "è¿™ç¯‡æ–‡ç« æ¶‰åŠ{tech_area}é¢†åŸŸçš„æœ€æ–°å‘å±•ï¼Œå¯¹ç§‘æŠ€æ”¿ç­–åˆ¶å®šå…·æœ‰é‡è¦å‚è€ƒä»·å€¼ã€‚",
            "æ–‡ç« è®¨è®ºäº†{tech_area}çš„åˆ›æ–°åº”ç”¨ï¼Œå±•ç°äº†è‰¯å¥½çš„å®ç”¨æ€§å’Œå‘å±•å‰æ™¯ã€‚",
            "è¯¥å†…å®¹å…³æ³¨{tech_area}çš„æ”¿ç­–å¯¼å‘ï¼Œä¸ºç›¸å…³å†³ç­–æä¾›äº†æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚",
            "æ–‡ç« åˆ†æäº†{tech_area}çš„æŠ€æœ¯è¶‹åŠ¿ï¼Œå¯¹äº§ä¸šå‘å±•å…·æœ‰æŒ‡å¯¼æ„ä¹‰ã€‚"
        ]
        
        # æ ¹æ®å…³é”®è¯ç¡®å®šæŠ€æœ¯é¢†åŸŸ
        if any(kw in title_lower for kw in ['ai', 'artificial intelligence', 'äººå·¥æ™ºèƒ½']):
            tech_area = "äººå·¥æ™ºèƒ½"
        elif any(kw in title_lower for kw in ['quantum', 'é‡å­']):
            tech_area = "é‡å­æŠ€æœ¯"
        elif any(kw in title_lower for kw in ['biotech', 'ç”Ÿç‰©æŠ€æœ¯', 'bio']):
            tech_area = "ç”Ÿç‰©æŠ€æœ¯"
        elif any(kw in title_lower for kw in ['semiconductor', 'åŠå¯¼ä½“', 'chip']):
            tech_area = "åŠå¯¼ä½“"
        elif any(kw in title_lower for kw in ['5g', '6g', 'network', 'ç½‘ç»œ']):
            tech_area = "é€šä¿¡ç½‘ç»œ"
        else:
            tech_area = "ç§‘æŠ€åˆ›æ–°"
        
        reasoning = random.choice(reasoning_templates).format(tech_area=tech_area)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„å…³é”®æ´å¯Ÿ
        key_insights = [
            f"{tech_area}é¢†åŸŸçš„æŠ€æœ¯å‘å±•è¶‹åŠ¿",
            "æ”¿ç­–å½±å“åˆ†æ",
            "äº§ä¸šåº”ç”¨å‰æ™¯è¯„ä¼°"
        ][:random.randint(1, 3)]
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„æ ‡ç­¾
        possible_tags = ['artificial_intelligence', 'quantum_technology', 'biotechnology', 
                        'semiconductor', '5g_6g_networks', 'cybersecurity', 'tech_policy', 
                        'industry_development', 'international_cooperation']
        tags = random.sample(possible_tags, random.randint(1, 3))
        
        evaluation = AIEvaluation(
            relevance_score=relevance_score,
            innovation_impact=innovation_impact,
            practicality=practicality,
            total_score=total_score,
            reasoning=reasoning,
            confidence=confidence,
            summary=f"æµ‹è¯•æ¨¡å¼æ¨¡æ‹Ÿè¯„ä¼°ï¼š{tech_area}ç›¸å…³å†…å®¹åˆ†æ",
            key_insights=key_insights,
            highlights=[article.title[:100]],
            tags=tags,
            detailed_analysis={
                "æŠ€æœ¯é¢†åŸŸ": tech_area,
                "è¯„ä¼°æ¨¡å¼": "æµ‹è¯•æ¨¡å¼",
                "ç›¸å…³æ€§è¯„åˆ†": relevance_score,
                "åˆ›æ–°å½±å“": innovation_impact,
                "å®ç”¨æ€§": practicality
            },
            recommendation_reason=f"åŸºäº{tech_area}é¢†åŸŸçš„é‡è¦æ€§å’Œæ–‡ç« å†…å®¹çš„ç›¸å…³æ€§",
            risk_assessment="æµ‹è¯•æ¨¡å¼ä¸‹çš„æ¨¡æ‹Ÿé£é™©è¯„ä¼°",
            implementation_suggestions=["å»ºè®®å…³æ³¨ç›¸å…³æ”¿ç­–åŠ¨å‘", "è·Ÿè¸ªæŠ€æœ¯å‘å±•è¶‹åŠ¿"]
        )
        
        # ç”Ÿæˆæ¨¡æ‹Ÿçš„åŸå§‹å“åº”
        raw_response = f"""æµ‹è¯•æ¨¡å¼æ¨¡æ‹Ÿå“åº”ï¼š
æ–‡ç« æ ‡é¢˜ï¼š{article.title}
æŠ€æœ¯é¢†åŸŸï¼š{tech_area}
è¯„ä¼°ç»“æœï¼š
- æ”¿ç­–ç›¸å…³æ€§ï¼š{relevance_score}/10
- åˆ›æ–°å½±å“ï¼š{innovation_impact}/10  
- å®ç”¨æ€§ï¼š{practicality}/10
- æ€»åˆ†ï¼š{total_score}/30
- ç½®ä¿¡åº¦ï¼š{confidence:.2f}
è¯„ä¼°ç†ç”±ï¼š{reasoning}

æ³¨æ„ï¼šè¿™æ˜¯æµ‹è¯•æ¨¡å¼ç”Ÿæˆçš„æ¨¡æ‹Ÿæ•°æ®ï¼Œæœªè°ƒç”¨çœŸå®AI APIã€‚"""
        
        return evaluation, raw_response

    def get_metrics(self) -> dict:
        """è·å–ç­›é€‰æŒ‡æ ‡"""
        metrics = self.metrics.get_performance_summary()
        
        # æ·»åŠ ç¼“å­˜ç»Ÿè®¡
        if self.cache:
            cache_stats = self.cache.get_stats()
            metrics.update(cache_stats)
        
        return metrics
    
    def reset_metrics(self):
        """é‡ç½®æŒ‡æ ‡"""
        self.metrics.reset()
    
    def clear_cache(self):
        """æ¸…ç©ºç¼“å­˜"""
        if self.cache:
            self.cache.clear()
    
    def cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        if self.cache:
            self.cache.cleanup_expired()
