"""
ç­›é€‰ç»“æœç¼“å­˜ç®¡ç†å™¨
ç”¨äºä¿å­˜å’Œæ¢å¤ç­›é€‰ç»“æœï¼Œå®ç°åº”ç”¨é‡å¯åçš„çŠ¶æ€æ¢å¤
"""
import json
import logging
from pathlib import Path
from datetime import datetime, timezone, timedelta
from typing import Optional, List, Dict, Any
from dataclasses import asdict

from ..models.news import NewsArticle
from ..filters.base import FilterChainResult, CombinedFilterResult, AIFilterResult, KeywordFilterResult, AIEvaluation

logger = logging.getLogger(__name__)


class FilterResultCache:
    """ç­›é€‰ç»“æœç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, cache_dir: str = "cache", expire_hours: int = 24):
        """
        åˆå§‹åŒ–ç­›é€‰ç»“æœç¼“å­˜
        
        Args:
            cache_dir: ç¼“å­˜ç›®å½•
            expire_hours: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰
        """
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.cache_file = self.cache_dir / "filter_results.json"
        self.expire_hours = expire_hours
        
        logger.info(f"ç­›é€‰ç»“æœç¼“å­˜åˆå§‹åŒ–: {self.cache_file}")
    
    def save_filter_result(self, 
                          filtered_articles: List[NewsArticle], 
                          filter_result: Optional[FilterChainResult] = None,
                          session_id: str = "default") -> bool:
        """
        ä¿å­˜ç­›é€‰ç»“æœ
        
        Args:
            filtered_articles: ç­›é€‰åçš„æ–‡ç« åˆ—è¡¨
            filter_result: å®Œæ•´çš„ç­›é€‰ç»“æœå¯¹è±¡
            session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒçš„ç­›é€‰ä¼šè¯
            
        Returns:
            æ˜¯å¦ä¿å­˜æˆåŠŸ
        """
        try:
            # å‡†å¤‡ç¼“å­˜æ•°æ®
            cache_data = {
                "session_id": session_id,
                "saved_at": datetime.now(timezone.utc).isoformat(),
                "filtered_articles": [self._serialize_article(article) for article in filtered_articles],
                "filter_result": self._serialize_filter_result(filter_result) if filter_result else None,
                "article_count": len(filtered_articles),
                "metadata": {
                    "version": "1.0",
                    "app_name": "news_selector"
                }
            }
            
            # ä¿å­˜åˆ°æ–‡ä»¶
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ç­›é€‰ç»“æœå·²ä¿å­˜: {len(filtered_articles)}ç¯‡æ–‡ç« , ä¼šè¯ID: {session_id}")
            print(f"ğŸ’¾ ç­›é€‰ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜: {len(filtered_articles)}ç¯‡æ–‡ç« ")
            return True
            
        except Exception as e:
            logger.error(f"ä¿å­˜ç­›é€‰ç»“æœå¤±è´¥: {e}")
            print(f"âŒ ä¿å­˜ç­›é€‰ç»“æœå¤±è´¥: {e}")
            return False
    
    def load_filter_result(self, session_id: str = "default") -> Optional[Dict[str, Any]]:
        """
        åŠ è½½ç­›é€‰ç»“æœ
        
        Args:
            session_id: ä¼šè¯ID
            
        Returns:
            ç­›é€‰ç»“æœæ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨æˆ–è¿‡æœŸåˆ™è¿”å›None
        """
        try:
            if not self.cache_file.exists():
                logger.debug("ç­›é€‰ç»“æœç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨")
                return None
            
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            # æ£€æŸ¥ä¼šè¯ID
            if cache_data.get("session_id") != session_id:
                logger.debug(f"ä¼šè¯IDä¸åŒ¹é…: æœŸæœ›{session_id}, å®é™…{cache_data.get('session_id')}")
                return None
            
            # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            saved_at = datetime.fromisoformat(cache_data["saved_at"])
            now = datetime.now(timezone.utc)
            age_hours = (now - saved_at).total_seconds() / 3600
            
            if age_hours > self.expire_hours:
                logger.info(f"ç­›é€‰ç»“æœç¼“å­˜å·²è¿‡æœŸ: {age_hours:.1f}å°æ—¶ > {self.expire_hours}å°æ—¶")
                self.clear_cache()
                return None
            
            # ååºåˆ—åŒ–æ–‡ç« åˆ—è¡¨
            filtered_articles = [
                self._deserialize_article(article_data) 
                for article_data in cache_data.get("filtered_articles", [])
            ]
            
            # ååºåˆ—åŒ–ç­›é€‰ç»“æœ
            filter_result = None
            if cache_data.get("filter_result"):
                filter_result = self._deserialize_filter_result(cache_data["filter_result"])
            
            logger.info(f"ç­›é€‰ç»“æœå·²åŠ è½½: {len(filtered_articles)}ç¯‡æ–‡ç« , ç¼“å­˜æ—¶é—´: {age_hours:.1f}å°æ—¶å‰")
            print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½ç­›é€‰ç»“æœ: {len(filtered_articles)}ç¯‡æ–‡ç«  (ç¼“å­˜æ—¶é—´: {age_hours:.1f}å°æ—¶å‰)")
            
            return {
                "filtered_articles": filtered_articles,
                "filter_result": filter_result,
                "saved_at": saved_at,
                "age_hours": age_hours,
                "article_count": len(filtered_articles)
            }
            
        except Exception as e:
            logger.error(f"åŠ è½½ç­›é€‰ç»“æœå¤±è´¥: {e}")
            print(f"âŒ åŠ è½½ç­›é€‰ç»“æœå¤±è´¥: {e}")
            return None
    
    def has_cached_result(self, session_id: str = "default") -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„ç­›é€‰ç»“æœ
        
        Args:
            session_id: ä¼šè¯ID
            
        Returns:
            æ˜¯å¦æœ‰æœ‰æ•ˆçš„ç¼“å­˜ç»“æœ
        """
        return self.load_filter_result(session_id) is not None
    
    def clear_cache(self, session_id: Optional[str] = None) -> bool:
        """
        æ¸…é™¤ç¼“å­˜
        
        Args:
            session_id: ä¼šè¯IDï¼Œå¦‚æœä¸ºNoneåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
            
        Returns:
            æ˜¯å¦æ¸…é™¤æˆåŠŸ
        """
        try:
            if session_id is None:
                # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
                if self.cache_file.exists():
                    self.cache_file.unlink()
                    logger.info("æ‰€æœ‰ç­›é€‰ç»“æœç¼“å­˜å·²æ¸…é™¤")
                    print("ğŸ—‘ï¸ ç­›é€‰ç»“æœç¼“å­˜å·²æ¸…é™¤")
                return True
            else:
                # æ¸…é™¤ç‰¹å®šä¼šè¯çš„ç¼“å­˜ï¼ˆå½“å‰å®ç°ä¸­ç­‰åŒäºæ¸…é™¤æ‰€æœ‰ï¼‰
                return self.clear_cache()
                
        except Exception as e:
            logger.error(f"æ¸…é™¤ç­›é€‰ç»“æœç¼“å­˜å¤±è´¥: {e}")
            print(f"âŒ æ¸…é™¤ç­›é€‰ç»“æœç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_cache_info(self) -> Optional[Dict[str, Any]]:
        """
        è·å–ç¼“å­˜ä¿¡æ¯
        
        Returns:
            ç¼“å­˜ä¿¡æ¯å­—å…¸
        """
        try:
            if not self.cache_file.exists():
                return None
            
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            
            saved_at = datetime.fromisoformat(cache_data["saved_at"])
            now = datetime.now(timezone.utc)
            age_hours = (now - saved_at).total_seconds() / 3600
            
            return {
                "session_id": cache_data.get("session_id"),
                "saved_at": saved_at,
                "age_hours": age_hours,
                "article_count": cache_data.get("article_count", 0),
                "is_expired": age_hours > self.expire_hours,
                "file_size": self.cache_file.stat().st_size,
                "file_path": str(self.cache_file)
            }
            
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _serialize_article(self, article: NewsArticle) -> Dict[str, Any]:
        """åºåˆ—åŒ–æ–‡ç« å¯¹è±¡"""
        return {
            "id": article.id,
            "title": article.title,
            "summary": article.summary,
            "content": article.content,
            "url": article.url,
            "published": article.published.isoformat() if article.published else None,
            "updated": article.updated.isoformat() if article.updated else None,
            "feed_title": getattr(article, 'feed_title', None),
            "is_read": getattr(article, 'is_read', False),
            "is_starred": getattr(article, 'is_starred', False),
            "tags": getattr(article, 'tags', [])
        }
    
    def _deserialize_article(self, data: Dict[str, Any]) -> NewsArticle:
        """ååºåˆ—åŒ–æ–‡ç« å¯¹è±¡"""
        article = NewsArticle(
            id=data["id"],
            title=data["title"],
            summary=data["summary"],
            content=data["content"],
            url=data["url"],
            published=datetime.fromisoformat(data["published"]) if data.get("published") else None,
            updated=datetime.fromisoformat(data["updated"]) if data.get("updated") else None
        )
        
        # è®¾ç½®é¢å¤–å±æ€§
        if data.get("feed_title"):
            article.feed_title = data["feed_title"]
        if data.get("is_read") is not None:
            article.is_read = data["is_read"]
        if data.get("is_starred") is not None:
            article.is_starred = data["is_starred"]
        if data.get("tags"):
            article.tags = data["tags"]
        
        return article
    
    def _serialize_filter_result(self, filter_result: FilterChainResult) -> Dict[str, Any]:
        """åºåˆ—åŒ–ç­›é€‰ç»“æœå¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            return {
                "total_articles": filter_result.total_articles,
                "final_selected_count": filter_result.final_selected_count,
                "processing_start_time": filter_result.processing_start_time.isoformat() if filter_result.processing_start_time else None,
                "processing_end_time": filter_result.processing_end_time.isoformat() if filter_result.processing_end_time else None,
                "total_processing_time": filter_result.total_processing_time,
                "keyword_filtered_count": getattr(filter_result, 'keyword_filtered_count', 0),
                "ai_filtered_count": getattr(filter_result, 'ai_filtered_count', 0),
                # æ³¨æ„ï¼šä¸åºåˆ—åŒ–selected_articlesï¼Œå› ä¸ºå®ƒä»¬å·²ç»åœ¨filtered_articlesä¸­
                "has_selected_articles": bool(filter_result.selected_articles)
            }
        except Exception as e:
            logger.warning(f"åºåˆ—åŒ–ç­›é€‰ç»“æœå¤±è´¥: {e}")
            return {}
    
    def _deserialize_filter_result(self, data: Dict[str, Any]) -> Optional[FilterChainResult]:
        """ååºåˆ—åŒ–ç­›é€‰ç»“æœå¯¹è±¡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        try:
            # åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„FilterChainResultå¯¹è±¡
            # æ³¨æ„ï¼šè¿™é‡Œä¸åŒ…å«selected_articlesï¼Œå› ä¸ºå®ƒä»¬ä¼šä»filtered_articlesé‡å»º
            filter_result = FilterChainResult(
                total_articles=data.get("total_articles", 0),
                processing_start_time=datetime.fromisoformat(data["processing_start_time"]) if data.get("processing_start_time") else datetime.now()
            )
            
            filter_result.final_selected_count = data.get("final_selected_count", 0)
            filter_result.processing_end_time = datetime.fromisoformat(data["processing_end_time"]) if data.get("processing_end_time") else datetime.now()
            filter_result.total_processing_time = data.get("total_processing_time", 0.0)
            filter_result.keyword_filtered_count = data.get("keyword_filtered_count", 0)
            filter_result.ai_filtered_count = data.get("ai_filtered_count", 0)
            
            return filter_result
            
        except Exception as e:
            logger.warning(f"ååºåˆ—åŒ–ç­›é€‰ç»“æœå¤±è´¥: {e}")
            return None


# å…¨å±€ç­›é€‰ç»“æœç¼“å­˜å®ä¾‹
_filter_result_cache = None


def get_filter_result_cache() -> FilterResultCache:
    """è·å–å…¨å±€ç­›é€‰ç»“æœç¼“å­˜å®ä¾‹"""
    global _filter_result_cache
    if _filter_result_cache is None:
        # ä½¿ç”¨é¡¹ç›®ç»Ÿä¸€çš„ç¼“å­˜ç›®å½•é…ç½®
        from ..config.settings import Settings
        settings = Settings()
        cache_dir = settings.app.cache_dir
        _filter_result_cache = FilterResultCache(cache_dir=cache_dir)
    return _filter_result_cache
